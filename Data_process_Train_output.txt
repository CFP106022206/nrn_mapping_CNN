2023-11-17 10:28:04.925112: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-11-17 10:28:04.927590: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.
2023-11-17 10:28:04.958709: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2023-11-17 10:28:04.959032: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2023-11-17 10:28:04.959782: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2023-11-17 10:28:04.965919: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.
2023-11-17 10:28:04.966295: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-11-17 10:28:06.635383: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-11-17 10:28:15.907542: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-11-17 10:28:15.908580: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2211] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...

Collecting 3-View Data Numpy Array..

 Resolutions: (3, 50, 50)

Collecting 3-View Data Numpy Array..

 Resolutions: (3, 50, 50)

Train data: 1003 
Valid data: 112 
Test data: 100
Total: 1003
Positive: 512 (51.05% of total)

Balanced Weight in: 
 [0 1] 
 [1.02138493 0.97949219]
UpSampling: After Augmentation:
True Label/Total in X_train:
 512 / 1024
X_train shape: (32768, 50, 50, 3) (32768, 50, 50, 3)
y_train shape: 32768
X_val shape: (112, 50, 50, 3) (112, 50, 50, 3)
y_val shape: 112
X_test shape: (100, 50, 50, 3) (100, 50, 50, 3)
y_test shape: 100
Model: "model"
__________________________________________________________________________________________________
 Layer (type)                Output Shape                 Param #   Connected to                  
==================================================================================================
 EM (InputLayer)             [(None, 50, 50, 3)]          0         []                            
                                                                                                  
 FC (InputLayer)             [(None, 50, 50, 3)]          0         []                            
                                                                                                  
 Shared_Conv1 (Conv2D)       (None, 48, 48, 16)           448       ['EM[0][0]',                  
                                                                     'FC[0][0]']                  
                                                                                                  
 Shared_BN1 (BatchNormaliza  (None, 48, 48, 16)           64        ['Shared_Conv1[0][0]',        
 tion)                                                               'Shared_Conv1[1][0]']        
                                                                                                  
 Shared_Activation1 (Activa  (None, 48, 48, 16)           0         ['Shared_BN1[0][0]',          
 tion)                                                               'Shared_BN1[1][0]']          
                                                                                                  
 max_pooling2d (MaxPooling2  (None, 24, 24, 16)           0         ['Shared_Activation1[0][0]']  
 D)                                                                                               
                                                                                                  
 max_pooling2d_1 (MaxPoolin  (None, 24, 24, 16)           0         ['Shared_Activation1[1][0]']  
 g2D)                                                                                             
                                                                                                  
 Shared_Conv2 (Conv2D)       (None, 22, 22, 32)           4640      ['max_pooling2d[0][0]',       
                                                                     'max_pooling2d_1[0][0]']     
                                                                                                  
 Shared_BN2 (BatchNormaliza  (None, 22, 22, 32)           128       ['Shared_Conv2[0][0]',        
 tion)                                                               'Shared_Conv2[1][0]']        
                                                                                                  
 Shared_Activation2 (Activa  (None, 22, 22, 32)           0         ['Shared_BN2[0][0]',          
 tion)                                                               'Shared_BN2[1][0]']          
                                                                                                  
 Shared_pool1 (MaxPooling2D  (None, 11, 11, 32)           0         ['Shared_Activation2[0][0]',  
 )                                                                   'Shared_Activation2[1][0]']  
                                                                                                  
 Shared_Conv3 (Conv2D)       (None, 9, 9, 48)             13872     ['Shared_pool1[0][0]',        
                                                                     'Shared_pool1[1][0]']        
                                                                                                  
 Shared_BN3 (BatchNormaliza  (None, 9, 9, 48)             192       ['Shared_Conv3[0][0]',        
 tion)                                                               'Shared_Conv3[1][0]']        
                                                                                                  
 Shared_Activation3 (Activa  (None, 9, 9, 48)             0         ['Shared_BN3[0][0]',          
 tion)                                                               'Shared_BN3[1][0]']          
                                                                                                  
 Shared_Conv4 (Conv2D)       (None, 7, 7, 64)             27712     ['Shared_Activation3[0][0]',  
                                                                     'Shared_Activation3[1][0]']  
                                                                                                  
 Shared_BN4 (BatchNormaliza  (None, 7, 7, 64)             256       ['Shared_Conv4[0][0]',        
 tion)                                                               'Shared_Conv4[1][0]']        
                                                                                                  
 Shared_Activation4 (Activa  (None, 7, 7, 64)             0         ['Shared_BN4[0][0]',          
 tion)                                                               'Shared_BN4[1][0]']          
                                                                                                  
 Shared_pool2 (MaxPooling2D  (None, 3, 3, 64)             0         ['Shared_Activation4[0][0]',  
 )                                                                   'Shared_Activation4[1][0]']  
                                                                                                  
 flatten (Flatten)           (None, 576)                  0         ['Shared_pool2[0][0]']        
                                                                                                  
 flatten_1 (Flatten)         (None, 576)                  0         ['Shared_pool2[1][0]']        
                                                                                                  
 concatenate (Concatenate)   (None, 1152)                 0         ['flatten[0][0]',             
                                                                     'flatten_1[0][0]']           
                                                                                                  
 dropout (Dropout)           (None, 1152)                 0         ['concatenate[0][0]']         
                                                                                                  
 dense (Dense)               (None, 128)                  147584    ['dropout[0][0]']             
                                                                                                  
 batch_normalization (Batch  (None, 128)                  512       ['dense[0][0]']               
 Normalization)                                                                                   
                                                                                                  
 activation (Activation)     (None, 128)                  0         ['batch_normalization[0][0]'] 
                                                                                                  
 dense_1 (Dense)             (None, 1)                    129       ['activation[0][0]']          
                                                                                                  
==================================================================================================
Total params: 195537 (763.82 KB)
Trainable params: 194961 (761.57 KB)
Non-trainable params: 576 (2.25 KB)
__________________________________________________________________________________________________

Use Callbacks: [<keras.src.callbacks.ModelCheckpoint object at 0x7fe5258eec80>]
Epoch 1/40

Epoch 1: val_loss improved from inf to 0.22954, saving model to ./Annotator_Model/Annotator_D1-D6_0.h5
/cluster/home/ming/.local/lib/python3.10/site-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.
  saving_api.save_model(
256/256 - 21s - loss: 0.1757 - Bi-Acc: 0.6277 - val_loss: 0.2295 - val_Bi-Acc: 0.5446 - 21s/epoch - 84ms/step
Epoch 2/40

Epoch 2: val_loss improved from 0.22954 to 0.13176, saving model to ./Annotator_Model/Annotator_D1-D6_0.h5
256/256 - 20s - loss: 0.1288 - Bi-Acc: 0.7469 - val_loss: 0.1318 - val_Bi-Acc: 0.7054 - 20s/epoch - 77ms/step
Epoch 3/40

Epoch 3: val_loss did not improve from 0.13176
256/256 - 20s - loss: 0.1143 - Bi-Acc: 0.7852 - val_loss: 0.1382 - val_Bi-Acc: 0.7679 - 20s/epoch - 77ms/step
Epoch 4/40

Epoch 4: val_loss improved from 0.13176 to 0.12668, saving model to ./Annotator_Model/Annotator_D1-D6_0.h5
256/256 - 20s - loss: 0.1023 - Bi-Acc: 0.8157 - val_loss: 0.1267 - val_Bi-Acc: 0.7768 - 20s/epoch - 76ms/step
Epoch 5/40

Epoch 5: val_loss improved from 0.12668 to 0.11056, saving model to ./Annotator_Model/Annotator_D1-D6_0.h5
256/256 - 19s - loss: 0.0920 - Bi-Acc: 0.8426 - val_loss: 0.1106 - val_Bi-Acc: 0.7946 - 19s/epoch - 76ms/step
Epoch 6/40

Epoch 6: val_loss did not improve from 0.11056
256/256 - 19s - loss: 0.0833 - Bi-Acc: 0.8586 - val_loss: 0.1151 - val_Bi-Acc: 0.8214 - 19s/epoch - 76ms/step
Epoch 7/40

Epoch 7: val_loss did not improve from 0.11056
256/256 - 20s - loss: 0.0768 - Bi-Acc: 0.8730 - val_loss: 0.1480 - val_Bi-Acc: 0.7768 - 20s/epoch - 77ms/step
Epoch 8/40

Epoch 8: val_loss did not improve from 0.11056
256/256 - 20s - loss: 0.0713 - Bi-Acc: 0.8829 - val_loss: 0.1291 - val_Bi-Acc: 0.7946 - 20s/epoch - 77ms/step
Epoch 9/40

Epoch 9: val_loss did not improve from 0.11056
256/256 - 19s - loss: 0.0655 - Bi-Acc: 0.8950 - val_loss: 0.1226 - val_Bi-Acc: 0.8482 - 19s/epoch - 76ms/step
Epoch 10/40

Epoch 10: val_loss did not improve from 0.11056
256/256 - 19s - loss: 0.0605 - Bi-Acc: 0.9023 - val_loss: 0.1546 - val_Bi-Acc: 0.7589 - 19s/epoch - 76ms/step
Epoch 11/40

Epoch 11: val_loss did not improve from 0.11056
256/256 - 20s - loss: 0.0569 - Bi-Acc: 0.9103 - val_loss: 0.1143 - val_Bi-Acc: 0.8571 - 20s/epoch - 77ms/step
Epoch 12/40

Epoch 12: val_loss improved from 0.11056 to 0.10395, saving model to ./Annotator_Model/Annotator_D1-D6_0.h5
256/256 - 20s - loss: 0.0536 - Bi-Acc: 0.9142 - val_loss: 0.1040 - val_Bi-Acc: 0.8571 - 20s/epoch - 76ms/step
Epoch 13/40

Epoch 13: val_loss did not improve from 0.10395
256/256 - 20s - loss: 0.0508 - Bi-Acc: 0.9198 - val_loss: 0.1447 - val_Bi-Acc: 0.8214 - 20s/epoch - 77ms/step
Epoch 14/40

Epoch 14: val_loss did not improve from 0.10395
256/256 - 20s - loss: 0.0467 - Bi-Acc: 0.9274 - val_loss: 0.1623 - val_Bi-Acc: 0.7857 - 20s/epoch - 78ms/step
Epoch 15/40

Epoch 15: val_loss did not improve from 0.10395
256/256 - 20s - loss: 0.0457 - Bi-Acc: 0.9269 - val_loss: 0.1652 - val_Bi-Acc: 0.8036 - 20s/epoch - 78ms/step
Epoch 16/40

Epoch 16: val_loss did not improve from 0.10395
256/256 - 20s - loss: 0.0420 - Bi-Acc: 0.9363 - val_loss: 0.1415 - val_Bi-Acc: 0.8571 - 20s/epoch - 77ms/step
Epoch 17/40

Epoch 17: val_loss did not improve from 0.10395
256/256 - 20s - loss: 0.0405 - Bi-Acc: 0.9364 - val_loss: 0.1290 - val_Bi-Acc: 0.8482 - 20s/epoch - 77ms/step
Epoch 18/40

Epoch 18: val_loss did not improve from 0.10395
256/256 - 20s - loss: 0.0382 - Bi-Acc: 0.9396 - val_loss: 0.1450 - val_Bi-Acc: 0.8393 - 20s/epoch - 77ms/step
Epoch 19/40

Epoch 19: val_loss did not improve from 0.10395
256/256 - 19s - loss: 0.0353 - Bi-Acc: 0.9459 - val_loss: 0.1704 - val_Bi-Acc: 0.8571 - 19s/epoch - 76ms/step
Epoch 20/40

Epoch 20: val_loss did not improve from 0.10395
256/256 - 20s - loss: 0.0336 - Bi-Acc: 0.9478 - val_loss: 0.1936 - val_Bi-Acc: 0.7946 - 20s/epoch - 76ms/step
Epoch 21/40

Epoch 21: val_loss did not improve from 0.10395
256/256 - 20s - loss: 0.0335 - Bi-Acc: 0.9501 - val_loss: 0.1498 - val_Bi-Acc: 0.8839 - 20s/epoch - 78ms/step
Epoch 22/40

Epoch 22: val_loss did not improve from 0.10395
256/256 - 20s - loss: 0.0305 - Bi-Acc: 0.9542 - val_loss: 0.1829 - val_Bi-Acc: 0.8571 - 20s/epoch - 77ms/step
Epoch 23/40

Epoch 23: val_loss did not improve from 0.10395
256/256 - 20s - loss: 0.0293 - Bi-Acc: 0.9565 - val_loss: 0.2697 - val_Bi-Acc: 0.8036 - 20s/epoch - 79ms/step
Epoch 24/40

Epoch 24: val_loss did not improve from 0.10395
256/256 - 20s - loss: 0.0297 - Bi-Acc: 0.9557 - val_loss: 0.1776 - val_Bi-Acc: 0.8304 - 20s/epoch - 77ms/step
Epoch 25/40

Epoch 25: val_loss did not improve from 0.10395
256/256 - 19s - loss: 0.0288 - Bi-Acc: 0.9577 - val_loss: 0.1884 - val_Bi-Acc: 0.8393 - 19s/epoch - 76ms/step
Epoch 26/40

Epoch 26: val_loss did not improve from 0.10395
256/256 - 19s - loss: 0.0269 - Bi-Acc: 0.9616 - val_loss: 0.1973 - val_Bi-Acc: 0.8304 - 19s/epoch - 75ms/step
Epoch 27/40

Epoch 27: val_loss did not improve from 0.10395
256/256 - 19s - loss: 0.0248 - Bi-Acc: 0.9632 - val_loss: 0.2208 - val_Bi-Acc: 0.8571 - 19s/epoch - 76ms/step
Epoch 28/40

Epoch 28: val_loss did not improve from 0.10395
256/256 - 20s - loss: 0.0248 - Bi-Acc: 0.9622 - val_loss: 0.1795 - val_Bi-Acc: 0.8571 - 20s/epoch - 79ms/step
Epoch 29/40

Epoch 29: val_loss did not improve from 0.10395
256/256 - 20s - loss: 0.0239 - Bi-Acc: 0.9643 - val_loss: 0.1792 - val_Bi-Acc: 0.8482 - 20s/epoch - 77ms/step
Epoch 30/40

Epoch 30: val_loss did not improve from 0.10395
256/256 - 20s - loss: 0.0231 - Bi-Acc: 0.9653 - val_loss: 0.1913 - val_Bi-Acc: 0.8304 - 20s/epoch - 78ms/step
Epoch 31/40

Epoch 31: val_loss did not improve from 0.10395
256/256 - 20s - loss: 0.0223 - Bi-Acc: 0.9662 - val_loss: 0.1998 - val_Bi-Acc: 0.8393 - 20s/epoch - 77ms/step
Epoch 32/40

Epoch 32: val_loss did not improve from 0.10395
256/256 - 20s - loss: 0.0210 - Bi-Acc: 0.9691 - val_loss: 0.2269 - val_Bi-Acc: 0.8661 - 20s/epoch - 78ms/step
Epoch 33/40

Epoch 33: val_loss did not improve from 0.10395
256/256 - 20s - loss: 0.0232 - Bi-Acc: 0.9664 - val_loss: 0.2118 - val_Bi-Acc: 0.8839 - 20s/epoch - 77ms/step
Epoch 34/40

Epoch 34: val_loss did not improve from 0.10395
256/256 - 20s - loss: 0.0199 - Bi-Acc: 0.9702 - val_loss: 0.2044 - val_Bi-Acc: 0.8571 - 20s/epoch - 78ms/step
Epoch 35/40

Epoch 35: val_loss did not improve from 0.10395
256/256 - 20s - loss: 0.0204 - Bi-Acc: 0.9702 - val_loss: 0.1698 - val_Bi-Acc: 0.8929 - 20s/epoch - 76ms/step
Epoch 36/40

Epoch 36: val_loss did not improve from 0.10395
256/256 - 20s - loss: 0.0194 - Bi-Acc: 0.9720 - val_loss: 0.2742 - val_Bi-Acc: 0.8214 - 20s/epoch - 77ms/step
Epoch 37/40

Epoch 37: val_loss did not improve from 0.10395
256/256 - 20s - loss: 0.0197 - Bi-Acc: 0.9717 - val_loss: 0.2232 - val_Bi-Acc: 0.8571 - 20s/epoch - 77ms/step
Epoch 38/40

Epoch 38: val_loss did not improve from 0.10395
256/256 - 20s - loss: 0.0175 - Bi-Acc: 0.9748 - val_loss: 0.2011 - val_Bi-Acc: 0.8750 - 20s/epoch - 77ms/step
Epoch 39/40

Epoch 39: val_loss did not improve from 0.10395
256/256 - 20s - loss: 0.0176 - Bi-Acc: 0.9745 - val_loss: 0.1910 - val_Bi-Acc: 0.8571 - 20s/epoch - 77ms/step
Epoch 40/40

Epoch 40: val_loss did not improve from 0.10395
256/256 - 20s - loss: 0.0167 - Bi-Acc: 0.9765 - val_loss: 0.2008 - val_Bi-Acc: 0.8393 - 20s/epoch - 78ms/step
4/4 - 0s - 199ms/epoch - 50ms/step
Validation:

Confusion Matrix for 0
True Pos False Neg
[50 12]
False Pos True Neg
[ 4 46]
Precision: 0.9259259259259259
Recall: 0.8064516129032258
F1 Score for Neg: 0.851851851851852
F1 Score for Pos: 0.8620689655172414
4/4 - 0s - 64ms/epoch - 16ms/step
Test:

Confusion Matrix for 0
True Pos False Neg
[44  6]
False Pos True Neg
[ 3 47]
Precision: 0.9361702127659575
Recall: 0.88
F1 Score for Neg: 0.912621359223301
F1 Score for Pos: 0.9072164948453608

Saved
