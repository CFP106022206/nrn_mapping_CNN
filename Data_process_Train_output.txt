2024-03-14 12:58:06.126641: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-14 12:58:06.152954: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-03-14 12:58:06.153015: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-03-14 12:58:06.153741: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-03-14 12:58:06.157683: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-14 12:58:08.465395: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-03-14 12:58:24.864301: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-14 12:58:24.951218: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-14 12:58:24.951456: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-14 12:58:24.952312: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-14 12:58:24.952434: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-14 12:58:24.952699: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-14 12:58:25.023130: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-14 12:58:25.023370: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-14 12:58:25.023837: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-03-14 12:58:25.023905: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22463 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:00:10.0, compute capability: 8.6

Collecting 3-View Data Numpy Array..

 Resolutions: (3, 50, 50)

Collecting 3-View Data Numpy Array..

 Resolutions: (3, 50, 50)

Original Train data: 933 
Valid data: 165 
Test data: 121

Total(After exchange): 1866
Positive: 856 (45.87% of total)

Balanced Weight in:
 [0.92376238 1.08995327]
UpSampling: After label balancing:
True Label/Total in x_train:
 856 / 2020
x_train shape: (64640, 50, 50, 3) (64640, 50, 50, 3)
y_train shape: 64640
x_val shape: (165, 50, 50, 3) (165, 50, 50, 3)
y_val shape: 165
X_test shape: (121, 50, 50, 3) (121, 50, 50, 3)
y_test shape: 121
Model: "model"
__________________________________________________________________________________________________
 Layer (type)                Output Shape                 Param #   Connected to                  
==================================================================================================
 FC (InputLayer)             [(None, 50, 50, 3)]          0         []                            
                                                                                                  
 EM (InputLayer)             [(None, 50, 50, 3)]          0         []                            
                                                                                                  
 conv1 (Conv2D)              (None, 50, 50, 32)           896       ['FC[0][0]',                  
                                                                     'EM[0][0]']                  
                                                                                                  
 bn1 (BatchNormalization)    (None, 50, 50, 32)           128       ['conv1[0][0]',               
                                                                     'conv1[1][0]']               
                                                                                                  
 ac1 (Activation)            (None, 50, 50, 32)           0         ['bn1[0][0]',                 
                                                                     'bn1[1][0]']                 
                                                                                                  
 conv2 (Conv2D)              (None, 50, 50, 32)           9248      ['ac1[0][0]',                 
                                                                     'ac1[1][0]']                 
                                                                                                  
 bn2 (BatchNormalization)    (None, 50, 50, 32)           128       ['conv2[0][0]',               
                                                                     'conv2[1][0]']               
                                                                                                  
 ac2 (Activation)            (None, 50, 50, 32)           0         ['bn2[0][0]',                 
                                                                     'bn2[1][0]']                 
                                                                                                  
 pool1 (MaxPooling2D)        (None, 25, 25, 32)           0         ['ac2[0][0]',                 
                                                                     'ac2[1][0]']                 
                                                                                                  
 conv3 (Conv2D)              (None, 25, 25, 64)           18496     ['pool1[0][0]',               
                                                                     'pool1[1][0]']               
                                                                                                  
 bn3 (BatchNormalization)    (None, 25, 25, 64)           256       ['conv3[0][0]',               
                                                                     'conv3[1][0]']               
                                                                                                  
 ac3 (Activation)            (None, 25, 25, 64)           0         ['bn3[0][0]',                 
                                                                     'bn3[1][0]']                 
                                                                                                  
 conv4 (Conv2D)              (None, 25, 25, 64)           36928     ['ac3[0][0]',                 
                                                                     'ac3[1][0]']                 
                                                                                                  
 bn4 (BatchNormalization)    (None, 25, 25, 64)           256       ['conv4[0][0]',               
                                                                     'conv4[1][0]']               
                                                                                                  
 ac4 (Activation)            (None, 25, 25, 64)           0         ['bn4[0][0]',                 
                                                                     'bn4[1][0]']                 
                                                                                                  
 pool2 (MaxPooling2D)        (None, 12, 12, 64)           0         ['ac4[0][0]',                 
                                                                     'ac4[1][0]']                 
                                                                                                  
 dropout (Dropout)           (None, 12, 12, 64)           0         ['pool2[0][0]']               
                                                                                                  
 dropout_1 (Dropout)         (None, 12, 12, 64)           0         ['pool2[1][0]']               
                                                                                                  
 flatten (Flatten)           (None, 9216)                 0         ['dropout[0][0]']             
                                                                                                  
 flatten_1 (Flatten)         (None, 9216)                 0         ['dropout_1[0][0]']           
                                                                                                  
 concatenate (Concatenate)   (None, 18432)                0         ['flatten[0][0]',             
                                                                     'flatten_1[0][0]']           
                                                                                                  
 dropout_2 (Dropout)         (None, 18432)                0         ['concatenate[0][0]']         
                                                                                                  
 dense (Dense)               (None, 128)                  2359424   ['dropout_2[0][0]']           
                                                                                                  
 batch_normalization (Batch  (None, 128)                  512       ['dense[0][0]']               
 Normalization)                                                                                   
                                                                                                  
 activation (Activation)     (None, 128)                  0         ['batch_normalization[0][0]'] 
                                                                                                  
 dense_1 (Dense)             (None, 1)                    129       ['activation[0][0]']          
                                                                                                  
==================================================================================================
Total params: 2426401 (9.26 MB)
Trainable params: 2425761 (9.25 MB)
Non-trainable params: 640 (2.50 KB)
__________________________________________________________________________________________________

Use Callbacks: [<keras.src.callbacks.ModelCheckpoint object at 0x7f747b5fcee0>]
Epoch 1/300
2024-03-14 12:58:31.866864: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:961] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape inmodel/dropout_1/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer
2024-03-14 12:58:32.077528: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8904
2024-03-14 12:58:32.124982: I external/local_tsl/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory
2024-03-14 12:58:32.178260: I external/local_tsl/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory
2024-03-14 12:58:33.239096: I external/local_xla/xla/service/service.cc:168] XLA service 0x7f6a5c47ed50 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2024-03-14 12:58:33.239151: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 3090, Compute Capability 8.6
2024-03-14 12:58:33.242597: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1710392313.314552  174773 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.

Epoch 1: val_loss improved from inf to 0.19918, saving model to ./Annotator_Model/Annotator_D1-D6_9.h5
/cluster/home/ming/anaconda3/envs/ming/lib/python3.9/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.
  saving_api.save_model(
505/505 - 19s - loss: 0.2237 - Bi-Acc: 0.2797 - val_loss: 0.1992 - val_Bi-Acc: 0.1152 - 19s/epoch - 38ms/step
Epoch 2/300

Epoch 2: val_loss did not improve from 0.19918
505/505 - 15s - loss: 0.2013 - Bi-Acc: 0.2854 - val_loss: 0.2058 - val_Bi-Acc: 0.1030 - 15s/epoch - 30ms/step
Epoch 3/300

Epoch 3: val_loss improved from 0.19918 to 0.18397, saving model to ./Annotator_Model/Annotator_D1-D6_9.h5
505/505 - 16s - loss: 0.1888 - Bi-Acc: 0.2846 - val_loss: 0.1840 - val_Bi-Acc: 0.1273 - 16s/epoch - 31ms/step
Epoch 4/300

Epoch 4: val_loss did not improve from 0.18397
505/505 - 15s - loss: 0.1784 - Bi-Acc: 0.2920 - val_loss: 0.1870 - val_Bi-Acc: 0.1515 - 15s/epoch - 30ms/step
Epoch 5/300

Epoch 5: val_loss improved from 0.18397 to 0.17615, saving model to ./Annotator_Model/Annotator_D1-D6_9.h5
505/505 - 16s - loss: 0.1716 - Bi-Acc: 0.2957 - val_loss: 0.1762 - val_Bi-Acc: 0.1576 - 16s/epoch - 31ms/step
Epoch 6/300

Epoch 6: val_loss did not improve from 0.17615
505/505 - 15s - loss: 0.1656 - Bi-Acc: 0.3010 - val_loss: 0.1855 - val_Bi-Acc: 0.1394 - 15s/epoch - 30ms/step
Epoch 7/300

Epoch 7: val_loss improved from 0.17615 to 0.17263, saving model to ./Annotator_Model/Annotator_D1-D6_9.h5
505/505 - 16s - loss: 0.1610 - Bi-Acc: 0.3045 - val_loss: 0.1726 - val_Bi-Acc: 0.1576 - 16s/epoch - 31ms/step
Epoch 8/300

Epoch 8: val_loss did not improve from 0.17263
505/505 - 15s - loss: 0.1570 - Bi-Acc: 0.3102 - val_loss: 0.1834 - val_Bi-Acc: 0.1394 - 15s/epoch - 30ms/step
Epoch 9/300

Epoch 9: val_loss improved from 0.17263 to 0.17206, saving model to ./Annotator_Model/Annotator_D1-D6_9.h5
505/505 - 15s - loss: 0.1539 - Bi-Acc: 0.3129 - val_loss: 0.1721 - val_Bi-Acc: 0.1515 - 15s/epoch - 31ms/step
Epoch 10/300

Epoch 10: val_loss improved from 0.17206 to 0.16277, saving model to ./Annotator_Model/Annotator_D1-D6_9.h5
505/505 - 16s - loss: 0.1501 - Bi-Acc: 0.3187 - val_loss: 0.1628 - val_Bi-Acc: 0.2061 - 16s/epoch - 31ms/step
Epoch 11/300

Epoch 11: val_loss did not improve from 0.16277
505/505 - 15s - loss: 0.1474 - Bi-Acc: 0.3216 - val_loss: 0.1659 - val_Bi-Acc: 0.1939 - 15s/epoch - 30ms/step
Epoch 12/300

Epoch 12: val_loss improved from 0.16277 to 0.15369, saving model to ./Annotator_Model/Annotator_D1-D6_9.h5
505/505 - 16s - loss: 0.1450 - Bi-Acc: 0.3236 - val_loss: 0.1537 - val_Bi-Acc: 0.2061 - 16s/epoch - 31ms/step
Epoch 13/300

Epoch 13: val_loss improved from 0.15369 to 0.14937, saving model to ./Annotator_Model/Annotator_D1-D6_9.h5
505/505 - 16s - loss: 0.1427 - Bi-Acc: 0.3253 - val_loss: 0.1494 - val_Bi-Acc: 0.2242 - 16s/epoch - 31ms/step
Epoch 14/300

Epoch 14: val_loss did not improve from 0.14937
505/505 - 15s - loss: 0.1408 - Bi-Acc: 0.3279 - val_loss: 0.1572 - val_Bi-Acc: 0.2000 - 15s/epoch - 30ms/step
Epoch 15/300

Epoch 15: val_loss did not improve from 0.14937
505/505 - 15s - loss: 0.1382 - Bi-Acc: 0.3309 - val_loss: 0.1579 - val_Bi-Acc: 0.2121 - 15s/epoch - 30ms/step
Epoch 16/300

Epoch 16: val_loss improved from 0.14937 to 0.14459, saving model to ./Annotator_Model/Annotator_D1-D6_9.h5
505/505 - 16s - loss: 0.1367 - Bi-Acc: 0.3334 - val_loss: 0.1446 - val_Bi-Acc: 0.2303 - 16s/epoch - 31ms/step
Epoch 17/300

Epoch 17: val_loss did not improve from 0.14459
505/505 - 15s - loss: 0.1349 - Bi-Acc: 0.3343 - val_loss: 0.1546 - val_Bi-Acc: 0.2182 - 15s/epoch - 30ms/step
Epoch 18/300

Epoch 18: val_loss did not improve from 0.14459
505/505 - 15s - loss: 0.1329 - Bi-Acc: 0.3355 - val_loss: 0.1497 - val_Bi-Acc: 0.2182 - 15s/epoch - 30ms/step
Epoch 19/300

Epoch 19: val_loss did not improve from 0.14459
505/505 - 15s - loss: 0.1319 - Bi-Acc: 0.3363 - val_loss: 0.1487 - val_Bi-Acc: 0.2242 - 15s/epoch - 30ms/step
Epoch 20/300

Epoch 20: val_loss did not improve from 0.14459
505/505 - 15s - loss: 0.1299 - Bi-Acc: 0.3393 - val_loss: 0.1447 - val_Bi-Acc: 0.2303 - 15s/epoch - 30ms/step
Epoch 21/300

Epoch 21: val_loss improved from 0.14459 to 0.14331, saving model to ./Annotator_Model/Annotator_D1-D6_9.h5
505/505 - 15s - loss: 0.1287 - Bi-Acc: 0.3412 - val_loss: 0.1433 - val_Bi-Acc: 0.2364 - 15s/epoch - 31ms/step
Epoch 22/300

Epoch 22: val_loss improved from 0.14331 to 0.14071, saving model to ./Annotator_Model/Annotator_D1-D6_9.h5
505/505 - 16s - loss: 0.1276 - Bi-Acc: 0.3418 - val_loss: 0.1407 - val_Bi-Acc: 0.2364 - 16s/epoch - 31ms/step
Epoch 23/300

Epoch 23: val_loss improved from 0.14071 to 0.14060, saving model to ./Annotator_Model/Annotator_D1-D6_9.h5
505/505 - 15s - loss: 0.1259 - Bi-Acc: 0.3441 - val_loss: 0.1406 - val_Bi-Acc: 0.2364 - 15s/epoch - 31ms/step
Epoch 24/300

Epoch 24: val_loss improved from 0.14060 to 0.13892, saving model to ./Annotator_Model/Annotator_D1-D6_9.h5
505/505 - 16s - loss: 0.1250 - Bi-Acc: 0.3440 - val_loss: 0.1389 - val_Bi-Acc: 0.2364 - 16s/epoch - 31ms/step
Epoch 25/300

Epoch 25: val_loss improved from 0.13892 to 0.12757, saving model to ./Annotator_Model/Annotator_D1-D6_9.h5
505/505 - 16s - loss: 0.1241 - Bi-Acc: 0.3448 - val_loss: 0.1276 - val_Bi-Acc: 0.2545 - 16s/epoch - 31ms/step
Epoch 26/300

Epoch 26: val_loss did not improve from 0.12757
505/505 - 15s - loss: 0.1228 - Bi-Acc: 0.3475 - val_loss: 0.1328 - val_Bi-Acc: 0.2364 - 15s/epoch - 30ms/step
Epoch 27/300

Epoch 27: val_loss did not improve from 0.12757
505/505 - 15s - loss: 0.1221 - Bi-Acc: 0.3470 - val_loss: 0.1375 - val_Bi-Acc: 0.2424 - 15s/epoch - 30ms/step
Epoch 28/300

Epoch 28: val_loss did not improve from 0.12757
505/505 - 15s - loss: 0.1212 - Bi-Acc: 0.3483 - val_loss: 0.1305 - val_Bi-Acc: 0.2424 - 15s/epoch - 30ms/step
Epoch 29/300

Epoch 29: val_loss improved from 0.12757 to 0.12329, saving model to ./Annotator_Model/Annotator_D1-D6_9.h5
505/505 - 16s - loss: 0.1200 - Bi-Acc: 0.3502 - val_loss: 0.1233 - val_Bi-Acc: 0.2788 - 16s/epoch - 31ms/step
Epoch 30/300

Epoch 30: val_loss did not improve from 0.12329
505/505 - 15s - loss: 0.1194 - Bi-Acc: 0.3503 - val_loss: 0.1251 - val_Bi-Acc: 0.2667 - 15s/epoch - 30ms/step
Epoch 31/300

Epoch 31: val_loss improved from 0.12329 to 0.12184, saving model to ./Annotator_Model/Annotator_D1-D6_9.h5
505/505 - 16s - loss: 0.1184 - Bi-Acc: 0.3514 - val_loss: 0.1218 - val_Bi-Acc: 0.2667 - 16s/epoch - 31ms/step
Epoch 32/300

Epoch 32: val_loss did not improve from 0.12184
505/505 - 15s - loss: 0.1176 - Bi-Acc: 0.3531 - val_loss: 0.1239 - val_Bi-Acc: 0.2667 - 15s/epoch - 30ms/step
Epoch 33/300

Epoch 33: val_loss improved from 0.12184 to 0.11915, saving model to ./Annotator_Model/Annotator_D1-D6_9.h5
505/505 - 16s - loss: 0.1165 - Bi-Acc: 0.3544 - val_loss: 0.1192 - val_Bi-Acc: 0.2909 - 16s/epoch - 31ms/step
Epoch 34/300

Epoch 34: val_loss did not improve from 0.11915
505/505 - 15s - loss: 0.1161 - Bi-Acc: 0.3547 - val_loss: 0.1295 - val_Bi-Acc: 0.2485 - 15s/epoch - 30ms/step
Epoch 35/300

Epoch 35: val_loss did not improve from 0.11915
505/505 - 15s - loss: 0.1155 - Bi-Acc: 0.3550 - val_loss: 0.1235 - val_Bi-Acc: 0.2788 - 15s/epoch - 30ms/step
Epoch 36/300

Epoch 36: val_loss improved from 0.11915 to 0.11861, saving model to ./Annotator_Model/Annotator_D1-D6_9.h5
505/505 - 16s - loss: 0.1143 - Bi-Acc: 0.3570 - val_loss: 0.1186 - val_Bi-Acc: 0.2788 - 16s/epoch - 31ms/step
Epoch 37/300

Epoch 37: val_loss did not improve from 0.11861
505/505 - 15s - loss: 0.1143 - Bi-Acc: 0.3560 - val_loss: 0.1210 - val_Bi-Acc: 0.2727 - 15s/epoch - 30ms/step
Epoch 38/300

Epoch 38: val_loss improved from 0.11861 to 0.11313, saving model to ./Annotator_Model/Annotator_D1-D6_9.h5
505/505 - 16s - loss: 0.1134 - Bi-Acc: 0.3567 - val_loss: 0.1131 - val_Bi-Acc: 0.3152 - 16s/epoch - 31ms/step
Epoch 39/300

Epoch 39: val_loss did not improve from 0.11313
505/505 - 15s - loss: 0.1125 - Bi-Acc: 0.3578 - val_loss: 0.1169 - val_Bi-Acc: 0.3030 - 15s/epoch - 30ms/step
Epoch 40/300

Epoch 40: val_loss did not improve from 0.11313
505/505 - 15s - loss: 0.1118 - Bi-Acc: 0.3583 - val_loss: 0.1158 - val_Bi-Acc: 0.3030 - 15s/epoch - 30ms/step
Epoch 41/300

Epoch 41: val_loss improved from 0.11313 to 0.11307, saving model to ./Annotator_Model/Annotator_D1-D6_9.h5
505/505 - 16s - loss: 0.1111 - Bi-Acc: 0.3594 - val_loss: 0.1131 - val_Bi-Acc: 0.3091 - 16s/epoch - 31ms/step
Epoch 42/300

Epoch 42: val_loss did not improve from 0.11307
505/505 - 15s - loss: 0.1107 - Bi-Acc: 0.3592 - val_loss: 0.1166 - val_Bi-Acc: 0.2970 - 15s/epoch - 30ms/step
Epoch 43/300

Epoch 43: val_loss did not improve from 0.11307
505/505 - 15s - loss: 0.1100 - Bi-Acc: 0.3612 - val_loss: 0.1132 - val_Bi-Acc: 0.3091 - 15s/epoch - 30ms/step
Epoch 44/300

Epoch 44: val_loss did not improve from 0.11307
505/505 - 15s - loss: 0.1093 - Bi-Acc: 0.3615 - val_loss: 0.1208 - val_Bi-Acc: 0.2788 - 15s/epoch - 30ms/step
Epoch 45/300

Epoch 45: val_loss improved from 0.11307 to 0.11099, saving model to ./Annotator_Model/Annotator_D1-D6_9.h5
505/505 - 15s - loss: 0.1081 - Bi-Acc: 0.3629 - val_loss: 0.1110 - val_Bi-Acc: 0.3212 - 15s/epoch - 31ms/step
Epoch 46/300

Epoch 46: val_loss did not improve from 0.11099
505/505 - 15s - loss: 0.1078 - Bi-Acc: 0.3626 - val_loss: 0.1139 - val_Bi-Acc: 0.3091 - 15s/epoch - 30ms/step
Epoch 47/300

Epoch 47: val_loss improved from 0.11099 to 0.10734, saving model to ./Annotator_Model/Annotator_D1-D6_9.h5
505/505 - 16s - loss: 0.1074 - Bi-Acc: 0.3634 - val_loss: 0.1073 - val_Bi-Acc: 0.3273 - 16s/epoch - 31ms/step
Epoch 48/300

Epoch 48: val_loss did not improve from 0.10734
505/505 - 15s - loss: 0.1070 - Bi-Acc: 0.3642 - val_loss: 0.1121 - val_Bi-Acc: 0.3030 - 15s/epoch - 30ms/step
Epoch 49/300

Epoch 49: val_loss did not improve from 0.10734
505/505 - 15s - loss: 0.1069 - Bi-Acc: 0.3632 - val_loss: 0.1112 - val_Bi-Acc: 0.3091 - 15s/epoch - 30ms/step
Epoch 50/300

Epoch 50: val_loss did not improve from 0.10734
505/505 - 15s - loss: 0.1056 - Bi-Acc: 0.3650 - val_loss: 0.1101 - val_Bi-Acc: 0.3212 - 15s/epoch - 30ms/step
Epoch 51/300

Epoch 51: val_loss did not improve from 0.10734
505/505 - 15s - loss: 0.1048 - Bi-Acc: 0.3655 - val_loss: 0.1167 - val_Bi-Acc: 0.2970 - 15s/epoch - 30ms/step
Epoch 52/300

Epoch 52: val_loss did not improve from 0.10734
505/505 - 15s - loss: 0.1047 - Bi-Acc: 0.3652 - val_loss: 0.1079 - val_Bi-Acc: 0.3152 - 15s/epoch - 30ms/step
Epoch 53/300

Epoch 53: val_loss did not improve from 0.10734
505/505 - 15s - loss: 0.1042 - Bi-Acc: 0.3661 - val_loss: 0.1134 - val_Bi-Acc: 0.3091 - 15s/epoch - 30ms/step
Epoch 54/300

Epoch 54: val_loss did not improve from 0.10734
505/505 - 15s - loss: 0.1032 - Bi-Acc: 0.3680 - val_loss: 0.1125 - val_Bi-Acc: 0.3091 - 15s/epoch - 30ms/step
Epoch 55/300

Epoch 55: val_loss improved from 0.10734 to 0.10560, saving model to ./Annotator_Model/Annotator_D1-D6_9.h5
505/505 - 16s - loss: 0.1029 - Bi-Acc: 0.3671 - val_loss: 0.1056 - val_Bi-Acc: 0.3212 - 16s/epoch - 31ms/step
Epoch 56/300

Epoch 56: val_loss did not improve from 0.10560
505/505 - 15s - loss: 0.1026 - Bi-Acc: 0.3676 - val_loss: 0.1067 - val_Bi-Acc: 0.3273 - 15s/epoch - 30ms/step
Epoch 57/300

Epoch 57: val_loss did not improve from 0.10560
505/505 - 15s - loss: 0.1018 - Bi-Acc: 0.3694 - val_loss: 0.1099 - val_Bi-Acc: 0.3152 - 15s/epoch - 30ms/step
Epoch 58/300

Epoch 58: val_loss did not improve from 0.10560
505/505 - 15s - loss: 0.1014 - Bi-Acc: 0.3691 - val_loss: 0.1120 - val_Bi-Acc: 0.3152 - 15s/epoch - 30ms/step
Epoch 59/300

Epoch 59: val_loss did not improve from 0.10560
505/505 - 15s - loss: 0.1012 - Bi-Acc: 0.3678 - val_loss: 0.1089 - val_Bi-Acc: 0.3152 - 15s/epoch - 30ms/step
Epoch 60/300

Epoch 60: val_loss improved from 0.10560 to 0.10542, saving model to ./Annotator_Model/Annotator_D1-D6_9.h5
505/505 - 16s - loss: 0.1002 - Bi-Acc: 0.3713 - val_loss: 0.1054 - val_Bi-Acc: 0.3212 - 16s/epoch - 31ms/step
Epoch 61/300

Epoch 61: val_loss did not improve from 0.10542
505/505 - 15s - loss: 0.0999 - Bi-Acc: 0.3709 - val_loss: 0.1061 - val_Bi-Acc: 0.3212 - 15s/epoch - 30ms/step
Epoch 62/300

Epoch 62: val_loss did not improve from 0.10542
505/505 - 15s - loss: 0.0994 - Bi-Acc: 0.3711 - val_loss: 0.1066 - val_Bi-Acc: 0.3212 - 15s/epoch - 30ms/step
Epoch 63/300

Epoch 63: val_loss improved from 0.10542 to 0.10496, saving model to ./Annotator_Model/Annotator_D1-D6_9.h5
505/505 - 16s - loss: 0.0992 - Bi-Acc: 0.3709 - val_loss: 0.1050 - val_Bi-Acc: 0.3333 - 16s/epoch - 31ms/step
Epoch 64/300

Epoch 64: val_loss did not improve from 0.10496
505/505 - 15s - loss: 0.0987 - Bi-Acc: 0.3716 - val_loss: 0.1056 - val_Bi-Acc: 0.3273 - 15s/epoch - 30ms/step
Epoch 65/300

Epoch 65: val_loss did not improve from 0.10496
505/505 - 15s - loss: 0.0984 - Bi-Acc: 0.3703 - val_loss: 0.1059 - val_Bi-Acc: 0.3212 - 15s/epoch - 30ms/step
Epoch 66/300

Epoch 66: val_loss improved from 0.10496 to 0.10417, saving model to ./Annotator_Model/Annotator_D1-D6_9.h5
505/505 - 16s - loss: 0.0977 - Bi-Acc: 0.3719 - val_loss: 0.1042 - val_Bi-Acc: 0.3273 - 16s/epoch - 32ms/step
Epoch 67/300

Epoch 67: val_loss did not improve from 0.10417
505/505 - 15s - loss: 0.0973 - Bi-Acc: 0.3733 - val_loss: 0.1071 - val_Bi-Acc: 0.3273 - 15s/epoch - 30ms/step
Epoch 68/300

Epoch 68: val_loss did not improve from 0.10417
505/505 - 15s - loss: 0.0969 - Bi-Acc: 0.3727 - val_loss: 0.1070 - val_Bi-Acc: 0.3273 - 15s/epoch - 30ms/step
Epoch 69/300

Epoch 69: val_loss did not improve from 0.10417
505/505 - 15s - loss: 0.0961 - Bi-Acc: 0.3732 - val_loss: 0.1098 - val_Bi-Acc: 0.3273 - 15s/epoch - 30ms/step
Epoch 70/300

Epoch 70: val_loss did not improve from 0.10417
505/505 - 15s - loss: 0.0958 - Bi-Acc: 0.3742 - val_loss: 0.1063 - val_Bi-Acc: 0.3333 - 15s/epoch - 30ms/step
Epoch 71/300

Epoch 71: val_loss did not improve from 0.10417
505/505 - 15s - loss: 0.0955 - Bi-Acc: 0.3736 - val_loss: 0.1080 - val_Bi-Acc: 0.3273 - 15s/epoch - 30ms/step
Epoch 72/300

Epoch 72: val_loss did not improve from 0.10417
505/505 - 15s - loss: 0.0953 - Bi-Acc: 0.3745 - val_loss: 0.1088 - val_Bi-Acc: 0.3152 - 15s/epoch - 30ms/step
Epoch 73/300

Epoch 73: val_loss improved from 0.10417 to 0.10233, saving model to ./Annotator_Model/Annotator_D1-D6_9.h5
505/505 - 16s - loss: 0.0947 - Bi-Acc: 0.3745 - val_loss: 0.1023 - val_Bi-Acc: 0.3394 - 16s/epoch - 31ms/step
Epoch 74/300

Epoch 74: val_loss did not improve from 0.10233
505/505 - 15s - loss: 0.0946 - Bi-Acc: 0.3742 - val_loss: 0.1041 - val_Bi-Acc: 0.3333 - 15s/epoch - 30ms/step
Epoch 75/300

Epoch 75: val_loss did not improve from 0.10233
505/505 - 15s - loss: 0.0939 - Bi-Acc: 0.3751 - val_loss: 0.1050 - val_Bi-Acc: 0.3333 - 15s/epoch - 30ms/step
Epoch 76/300

Epoch 76: val_loss did not improve from 0.10233
505/505 - 15s - loss: 0.0936 - Bi-Acc: 0.3757 - val_loss: 0.1075 - val_Bi-Acc: 0.3333 - 15s/epoch - 30ms/step
Epoch 77/300

Epoch 77: val_loss did not improve from 0.10233
505/505 - 15s - loss: 0.0937 - Bi-Acc: 0.3758 - val_loss: 0.1175 - val_Bi-Acc: 0.3030 - 15s/epoch - 30ms/step
Epoch 78/300

Epoch 78: val_loss did not improve from 0.10233
505/505 - 15s - loss: 0.0932 - Bi-Acc: 0.3759 - val_loss: 0.1041 - val_Bi-Acc: 0.3333 - 15s/epoch - 30ms/step
Epoch 79/300

Epoch 79: val_loss did not improve from 0.10233
505/505 - 15s - loss: 0.0924 - Bi-Acc: 0.3768 - val_loss: 0.1029 - val_Bi-Acc: 0.3273 - 15s/epoch - 30ms/step
Epoch 80/300

Epoch 80: val_loss did not improve from 0.10233
505/505 - 15s - loss: 0.0925 - Bi-Acc: 0.3764 - val_loss: 0.1056 - val_Bi-Acc: 0.3273 - 15s/epoch - 30ms/step
Epoch 81/300

Epoch 81: val_loss did not improve from 0.10233
505/505 - 15s - loss: 0.0923 - Bi-Acc: 0.3761 - val_loss: 0.1084 - val_Bi-Acc: 0.3212 - 15s/epoch - 30ms/step
Epoch 82/300

Epoch 82: val_loss did not improve from 0.10233
505/505 - 15s - loss: 0.0917 - Bi-Acc: 0.3773 - val_loss: 0.1115 - val_Bi-Acc: 0.3030 - 15s/epoch - 30ms/step
Epoch 83/300

Epoch 83: val_loss did not improve from 0.10233
505/505 - 15s - loss: 0.0909 - Bi-Acc: 0.3779 - val_loss: 0.1026 - val_Bi-Acc: 0.3394 - 15s/epoch - 30ms/step
Epoch 84/300

Epoch 84: val_loss did not improve from 0.10233
505/505 - 15s - loss: 0.0910 - Bi-Acc: 0.3781 - val_loss: 0.1048 - val_Bi-Acc: 0.3273 - 15s/epoch - 30ms/step
Epoch 85/300

Epoch 85: val_loss did not improve from 0.10233
505/505 - 15s - loss: 0.0908 - Bi-Acc: 0.3779 - val_loss: 0.1068 - val_Bi-Acc: 0.3273 - 15s/epoch - 30ms/step
Epoch 86/300

Epoch 86: val_loss improved from 0.10233 to 0.10083, saving model to ./Annotator_Model/Annotator_D1-D6_9.h5
505/505 - 16s - loss: 0.0902 - Bi-Acc: 0.3789 - val_loss: 0.1008 - val_Bi-Acc: 0.3455 - 16s/epoch - 31ms/step
Epoch 87/300

Epoch 87: val_loss improved from 0.10083 to 0.10054, saving model to ./Annotator_Model/Annotator_D1-D6_9.h5
505/505 - 16s - loss: 0.0897 - Bi-Acc: 0.3789 - val_loss: 0.1005 - val_Bi-Acc: 0.3515 - 16s/epoch - 32ms/step
Epoch 88/300

Epoch 88: val_loss did not improve from 0.10054
505/505 - 15s - loss: 0.0893 - Bi-Acc: 0.3782 - val_loss: 0.1037 - val_Bi-Acc: 0.3333 - 15s/epoch - 30ms/step
Epoch 89/300

Epoch 89: val_loss did not improve from 0.10054
505/505 - 15s - loss: 0.0895 - Bi-Acc: 0.3789 - val_loss: 0.1012 - val_Bi-Acc: 0.3394 - 15s/epoch - 30ms/step
Epoch 90/300

Epoch 90: val_loss did not improve from 0.10054
505/505 - 15s - loss: 0.0892 - Bi-Acc: 0.3793 - val_loss: 0.1028 - val_Bi-Acc: 0.3333 - 15s/epoch - 30ms/step
Epoch 91/300

Epoch 91: val_loss did not improve from 0.10054
505/505 - 15s - loss: 0.0887 - Bi-Acc: 0.3797 - val_loss: 0.1025 - val_Bi-Acc: 0.3273 - 15s/epoch - 30ms/step
Epoch 92/300

Epoch 92: val_loss did not improve from 0.10054
505/505 - 15s - loss: 0.0883 - Bi-Acc: 0.3802 - val_loss: 0.1039 - val_Bi-Acc: 0.3333 - 15s/epoch - 30ms/step
Epoch 93/300

Epoch 93: val_loss did not improve from 0.10054
505/505 - 15s - loss: 0.0884 - Bi-Acc: 0.3800 - val_loss: 0.1044 - val_Bi-Acc: 0.3212 - 15s/epoch - 30ms/step
Epoch 94/300

Epoch 94: val_loss did not improve from 0.10054
505/505 - 15s - loss: 0.0883 - Bi-Acc: 0.3801 - val_loss: 0.1034 - val_Bi-Acc: 0.3333 - 15s/epoch - 30ms/step
Epoch 95/300

Epoch 95: val_loss did not improve from 0.10054
505/505 - 15s - loss: 0.0877 - Bi-Acc: 0.3803 - val_loss: 0.1073 - val_Bi-Acc: 0.3091 - 15s/epoch - 30ms/step
Epoch 96/300

Epoch 96: val_loss did not improve from 0.10054
505/505 - 15s - loss: 0.0872 - Bi-Acc: 0.3812 - val_loss: 0.1028 - val_Bi-Acc: 0.3333 - 15s/epoch - 30ms/step
Epoch 97/300

Epoch 97: val_loss did not improve from 0.10054
505/505 - 15s - loss: 0.0876 - Bi-Acc: 0.3806 - val_loss: 0.1021 - val_Bi-Acc: 0.3394 - 15s/epoch - 30ms/step
Epoch 98/300

Epoch 98: val_loss did not improve from 0.10054
505/505 - 15s - loss: 0.0870 - Bi-Acc: 0.3804 - val_loss: 0.1026 - val_Bi-Acc: 0.3394 - 15s/epoch - 30ms/step
Epoch 99/300

Epoch 99: val_loss did not improve from 0.10054
505/505 - 15s - loss: 0.0866 - Bi-Acc: 0.3818 - val_loss: 0.1018 - val_Bi-Acc: 0.3455 - 15s/epoch - 30ms/step
Epoch 100/300

Epoch 100: val_loss improved from 0.10054 to 0.10043, saving model to ./Annotator_Model/Annotator_D1-D6_9.h5
505/505 - 16s - loss: 0.0858 - Bi-Acc: 0.3825 - val_loss: 0.1004 - val_Bi-Acc: 0.3455 - 16s/epoch - 31ms/step
Epoch 101/300

Epoch 101: val_loss did not improve from 0.10043
505/505 - 15s - loss: 0.0860 - Bi-Acc: 0.3814 - val_loss: 0.1033 - val_Bi-Acc: 0.3394 - 15s/epoch - 30ms/step
Epoch 102/300

Epoch 102: val_loss did not improve from 0.10043
505/505 - 15s - loss: 0.0858 - Bi-Acc: 0.3827 - val_loss: 0.1007 - val_Bi-Acc: 0.3455 - 15s/epoch - 30ms/step
Epoch 103/300

Epoch 103: val_loss did not improve from 0.10043
505/505 - 15s - loss: 0.0855 - Bi-Acc: 0.3826 - val_loss: 0.1068 - val_Bi-Acc: 0.3212 - 15s/epoch - 30ms/step
Epoch 104/300

Epoch 104: val_loss did not improve from 0.10043
505/505 - 15s - loss: 0.0856 - Bi-Acc: 0.3824 - val_loss: 0.1047 - val_Bi-Acc: 0.3273 - 15s/epoch - 30ms/step
Epoch 105/300

Epoch 105: val_loss did not improve from 0.10043
505/505 - 15s - loss: 0.0851 - Bi-Acc: 0.3822 - val_loss: 0.1019 - val_Bi-Acc: 0.3515 - 15s/epoch - 30ms/step
Epoch 106/300

Epoch 106: val_loss did not improve from 0.10043
505/505 - 15s - loss: 0.0849 - Bi-Acc: 0.3823 - val_loss: 0.1013 - val_Bi-Acc: 0.3455 - 15s/epoch - 30ms/step
Epoch 107/300

Epoch 107: val_loss did not improve from 0.10043
505/505 - 15s - loss: 0.0842 - Bi-Acc: 0.3831 - val_loss: 0.1025 - val_Bi-Acc: 0.3394 - 15s/epoch - 30ms/step
Epoch 108/300

Epoch 108: val_loss improved from 0.10043 to 0.09723, saving model to ./Annotator_Model/Annotator_D1-D6_9.h5
505/505 - 15s - loss: 0.0840 - Bi-Acc: 0.3842 - val_loss: 0.0972 - val_Bi-Acc: 0.3515 - 15s/epoch - 31ms/step
Epoch 109/300

Epoch 109: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0838 - Bi-Acc: 0.3839 - val_loss: 0.0996 - val_Bi-Acc: 0.3455 - 15s/epoch - 30ms/step
Epoch 110/300

Epoch 110: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0837 - Bi-Acc: 0.3839 - val_loss: 0.1026 - val_Bi-Acc: 0.3333 - 15s/epoch - 30ms/step
Epoch 111/300

Epoch 111: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0837 - Bi-Acc: 0.3836 - val_loss: 0.1000 - val_Bi-Acc: 0.3455 - 15s/epoch - 30ms/step
Epoch 112/300

Epoch 112: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0832 - Bi-Acc: 0.3841 - val_loss: 0.1013 - val_Bi-Acc: 0.3394 - 15s/epoch - 30ms/step
Epoch 113/300

Epoch 113: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0831 - Bi-Acc: 0.3840 - val_loss: 0.0992 - val_Bi-Acc: 0.3455 - 15s/epoch - 30ms/step
Epoch 114/300

Epoch 114: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0832 - Bi-Acc: 0.3836 - val_loss: 0.1007 - val_Bi-Acc: 0.3394 - 15s/epoch - 30ms/step
Epoch 115/300

Epoch 115: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0827 - Bi-Acc: 0.3846 - val_loss: 0.1034 - val_Bi-Acc: 0.3394 - 15s/epoch - 30ms/step
Epoch 116/300

Epoch 116: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0824 - Bi-Acc: 0.3847 - val_loss: 0.1017 - val_Bi-Acc: 0.3455 - 15s/epoch - 30ms/step
Epoch 117/300

Epoch 117: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0819 - Bi-Acc: 0.3849 - val_loss: 0.1028 - val_Bi-Acc: 0.3333 - 15s/epoch - 30ms/step
Epoch 118/300

Epoch 118: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0821 - Bi-Acc: 0.3847 - val_loss: 0.1022 - val_Bi-Acc: 0.3333 - 15s/epoch - 30ms/step
Epoch 119/300

Epoch 119: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0815 - Bi-Acc: 0.3851 - val_loss: 0.1061 - val_Bi-Acc: 0.3333 - 15s/epoch - 30ms/step
Epoch 120/300

Epoch 120: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0818 - Bi-Acc: 0.3852 - val_loss: 0.1005 - val_Bi-Acc: 0.3455 - 15s/epoch - 30ms/step
Epoch 121/300

Epoch 121: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0813 - Bi-Acc: 0.3856 - val_loss: 0.1030 - val_Bi-Acc: 0.3273 - 15s/epoch - 30ms/step
Epoch 122/300

Epoch 122: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0811 - Bi-Acc: 0.3858 - val_loss: 0.0991 - val_Bi-Acc: 0.3455 - 15s/epoch - 30ms/step
Epoch 123/300

Epoch 123: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0809 - Bi-Acc: 0.3858 - val_loss: 0.1036 - val_Bi-Acc: 0.3394 - 15s/epoch - 30ms/step
Epoch 124/300

Epoch 124: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0806 - Bi-Acc: 0.3860 - val_loss: 0.1009 - val_Bi-Acc: 0.3455 - 15s/epoch - 30ms/step
Epoch 125/300

Epoch 125: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0801 - Bi-Acc: 0.3860 - val_loss: 0.1035 - val_Bi-Acc: 0.3333 - 15s/epoch - 30ms/step
Epoch 126/300

Epoch 126: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0801 - Bi-Acc: 0.3863 - val_loss: 0.1007 - val_Bi-Acc: 0.3455 - 15s/epoch - 30ms/step
Epoch 127/300

Epoch 127: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0802 - Bi-Acc: 0.3855 - val_loss: 0.1033 - val_Bi-Acc: 0.3273 - 15s/epoch - 30ms/step
Epoch 128/300

Epoch 128: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0799 - Bi-Acc: 0.3867 - val_loss: 0.1067 - val_Bi-Acc: 0.3273 - 15s/epoch - 30ms/step
Epoch 129/300

Epoch 129: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0797 - Bi-Acc: 0.3861 - val_loss: 0.1042 - val_Bi-Acc: 0.3394 - 15s/epoch - 30ms/step
Epoch 130/300

Epoch 130: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0792 - Bi-Acc: 0.3869 - val_loss: 0.1019 - val_Bi-Acc: 0.3455 - 15s/epoch - 30ms/step
Epoch 131/300

Epoch 131: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0794 - Bi-Acc: 0.3867 - val_loss: 0.1033 - val_Bi-Acc: 0.3333 - 15s/epoch - 30ms/step
Epoch 132/300

Epoch 132: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0792 - Bi-Acc: 0.3869 - val_loss: 0.1017 - val_Bi-Acc: 0.3394 - 15s/epoch - 30ms/step
Epoch 133/300

Epoch 133: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0791 - Bi-Acc: 0.3868 - val_loss: 0.1056 - val_Bi-Acc: 0.3273 - 15s/epoch - 30ms/step
Epoch 134/300

Epoch 134: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0789 - Bi-Acc: 0.3873 - val_loss: 0.1022 - val_Bi-Acc: 0.3394 - 15s/epoch - 30ms/step
Epoch 135/300

Epoch 135: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0788 - Bi-Acc: 0.3872 - val_loss: 0.1049 - val_Bi-Acc: 0.3273 - 15s/epoch - 30ms/step
Epoch 136/300

Epoch 136: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0785 - Bi-Acc: 0.3867 - val_loss: 0.1092 - val_Bi-Acc: 0.3212 - 15s/epoch - 30ms/step
Epoch 137/300

Epoch 137: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0779 - Bi-Acc: 0.3882 - val_loss: 0.1067 - val_Bi-Acc: 0.3333 - 15s/epoch - 30ms/step
Epoch 138/300

Epoch 138: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0782 - Bi-Acc: 0.3872 - val_loss: 0.1051 - val_Bi-Acc: 0.3333 - 15s/epoch - 30ms/step
Epoch 139/300

Epoch 139: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0781 - Bi-Acc: 0.3879 - val_loss: 0.1064 - val_Bi-Acc: 0.3212 - 15s/epoch - 30ms/step
Epoch 140/300

Epoch 140: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0778 - Bi-Acc: 0.3875 - val_loss: 0.1029 - val_Bi-Acc: 0.3455 - 15s/epoch - 30ms/step
Epoch 141/300

Epoch 141: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0777 - Bi-Acc: 0.3884 - val_loss: 0.0994 - val_Bi-Acc: 0.3515 - 15s/epoch - 30ms/step
Epoch 142/300

Epoch 142: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0772 - Bi-Acc: 0.3883 - val_loss: 0.1036 - val_Bi-Acc: 0.3394 - 15s/epoch - 30ms/step
Epoch 143/300

Epoch 143: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0774 - Bi-Acc: 0.3879 - val_loss: 0.0987 - val_Bi-Acc: 0.3455 - 15s/epoch - 30ms/step
Epoch 144/300

Epoch 144: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0768 - Bi-Acc: 0.3888 - val_loss: 0.1013 - val_Bi-Acc: 0.3455 - 15s/epoch - 30ms/step
Epoch 145/300

Epoch 145: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0765 - Bi-Acc: 0.3886 - val_loss: 0.0986 - val_Bi-Acc: 0.3515 - 15s/epoch - 30ms/step
Epoch 146/300

Epoch 146: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0769 - Bi-Acc: 0.3877 - val_loss: 0.1037 - val_Bi-Acc: 0.3333 - 15s/epoch - 30ms/step
Epoch 147/300

Epoch 147: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0763 - Bi-Acc: 0.3884 - val_loss: 0.1052 - val_Bi-Acc: 0.3273 - 15s/epoch - 30ms/step
Epoch 148/300

Epoch 148: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0762 - Bi-Acc: 0.3889 - val_loss: 0.1095 - val_Bi-Acc: 0.3152 - 15s/epoch - 30ms/step
Epoch 149/300

Epoch 149: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0763 - Bi-Acc: 0.3888 - val_loss: 0.1011 - val_Bi-Acc: 0.3455 - 15s/epoch - 30ms/step
Epoch 150/300

Epoch 150: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0760 - Bi-Acc: 0.3890 - val_loss: 0.1045 - val_Bi-Acc: 0.3212 - 15s/epoch - 30ms/step
Epoch 151/300

Epoch 151: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0757 - Bi-Acc: 0.3885 - val_loss: 0.1027 - val_Bi-Acc: 0.3333 - 15s/epoch - 30ms/step
Epoch 152/300

Epoch 152: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0756 - Bi-Acc: 0.3888 - val_loss: 0.1026 - val_Bi-Acc: 0.3333 - 15s/epoch - 30ms/step
Epoch 153/300

Epoch 153: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0758 - Bi-Acc: 0.3889 - val_loss: 0.1024 - val_Bi-Acc: 0.3394 - 15s/epoch - 30ms/step
Epoch 154/300

Epoch 154: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0755 - Bi-Acc: 0.3889 - val_loss: 0.1061 - val_Bi-Acc: 0.3333 - 15s/epoch - 30ms/step
Epoch 155/300

Epoch 155: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0755 - Bi-Acc: 0.3892 - val_loss: 0.0973 - val_Bi-Acc: 0.3515 - 15s/epoch - 30ms/step
Epoch 156/300

Epoch 156: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0750 - Bi-Acc: 0.3894 - val_loss: 0.1035 - val_Bi-Acc: 0.3333 - 15s/epoch - 30ms/step
Epoch 157/300

Epoch 157: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0750 - Bi-Acc: 0.3891 - val_loss: 0.1039 - val_Bi-Acc: 0.3333 - 15s/epoch - 30ms/step
Epoch 158/300

Epoch 158: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0748 - Bi-Acc: 0.3899 - val_loss: 0.1024 - val_Bi-Acc: 0.3515 - 15s/epoch - 30ms/step
Epoch 159/300

Epoch 159: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0746 - Bi-Acc: 0.3899 - val_loss: 0.1032 - val_Bi-Acc: 0.3273 - 15s/epoch - 30ms/step
Epoch 160/300

Epoch 160: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0748 - Bi-Acc: 0.3894 - val_loss: 0.1006 - val_Bi-Acc: 0.3455 - 15s/epoch - 30ms/step
Epoch 161/300

Epoch 161: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0746 - Bi-Acc: 0.3902 - val_loss: 0.1021 - val_Bi-Acc: 0.3394 - 15s/epoch - 30ms/step
Epoch 162/300

Epoch 162: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0744 - Bi-Acc: 0.3899 - val_loss: 0.1005 - val_Bi-Acc: 0.3455 - 15s/epoch - 30ms/step
Epoch 163/300

Epoch 163: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0744 - Bi-Acc: 0.3899 - val_loss: 0.1001 - val_Bi-Acc: 0.3394 - 15s/epoch - 30ms/step
Epoch 164/300

Epoch 164: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0741 - Bi-Acc: 0.3903 - val_loss: 0.1009 - val_Bi-Acc: 0.3515 - 15s/epoch - 30ms/step
Epoch 165/300

Epoch 165: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0741 - Bi-Acc: 0.3892 - val_loss: 0.1008 - val_Bi-Acc: 0.3515 - 15s/epoch - 30ms/step
Epoch 166/300

Epoch 166: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0740 - Bi-Acc: 0.3901 - val_loss: 0.1056 - val_Bi-Acc: 0.3212 - 15s/epoch - 30ms/step
Epoch 167/300

Epoch 167: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0735 - Bi-Acc: 0.3904 - val_loss: 0.1026 - val_Bi-Acc: 0.3455 - 15s/epoch - 30ms/step
Epoch 168/300

Epoch 168: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0734 - Bi-Acc: 0.3906 - val_loss: 0.1020 - val_Bi-Acc: 0.3455 - 15s/epoch - 30ms/step
Epoch 169/300

Epoch 169: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0732 - Bi-Acc: 0.3907 - val_loss: 0.1023 - val_Bi-Acc: 0.3515 - 15s/epoch - 30ms/step
Epoch 170/300

Epoch 170: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0730 - Bi-Acc: 0.3905 - val_loss: 0.1040 - val_Bi-Acc: 0.3333 - 15s/epoch - 30ms/step
Epoch 171/300

Epoch 171: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0730 - Bi-Acc: 0.3903 - val_loss: 0.1018 - val_Bi-Acc: 0.3515 - 15s/epoch - 30ms/step
Epoch 172/300

Epoch 172: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0729 - Bi-Acc: 0.3908 - val_loss: 0.1038 - val_Bi-Acc: 0.3394 - 15s/epoch - 30ms/step
Epoch 173/300

Epoch 173: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0730 - Bi-Acc: 0.3910 - val_loss: 0.1070 - val_Bi-Acc: 0.3212 - 15s/epoch - 30ms/step
Epoch 174/300

Epoch 174: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0728 - Bi-Acc: 0.3908 - val_loss: 0.1053 - val_Bi-Acc: 0.3152 - 15s/epoch - 30ms/step
Epoch 175/300

Epoch 175: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0725 - Bi-Acc: 0.3911 - val_loss: 0.1055 - val_Bi-Acc: 0.3273 - 15s/epoch - 30ms/step
Epoch 176/300

Epoch 176: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0722 - Bi-Acc: 0.3909 - val_loss: 0.1024 - val_Bi-Acc: 0.3333 - 15s/epoch - 30ms/step
Epoch 177/300

Epoch 177: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0727 - Bi-Acc: 0.3904 - val_loss: 0.1068 - val_Bi-Acc: 0.3212 - 15s/epoch - 30ms/step
Epoch 178/300

Epoch 178: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0722 - Bi-Acc: 0.3915 - val_loss: 0.1072 - val_Bi-Acc: 0.3212 - 15s/epoch - 30ms/step
Epoch 179/300

Epoch 179: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0723 - Bi-Acc: 0.3909 - val_loss: 0.1040 - val_Bi-Acc: 0.3333 - 15s/epoch - 30ms/step
Epoch 180/300

Epoch 180: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0720 - Bi-Acc: 0.3916 - val_loss: 0.1024 - val_Bi-Acc: 0.3333 - 15s/epoch - 30ms/step
Epoch 181/300

Epoch 181: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0720 - Bi-Acc: 0.3911 - val_loss: 0.1074 - val_Bi-Acc: 0.3212 - 15s/epoch - 30ms/step
Epoch 182/300

Epoch 182: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0719 - Bi-Acc: 0.3913 - val_loss: 0.1021 - val_Bi-Acc: 0.3394 - 15s/epoch - 30ms/step
Epoch 183/300

Epoch 183: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0716 - Bi-Acc: 0.3916 - val_loss: 0.1022 - val_Bi-Acc: 0.3455 - 15s/epoch - 30ms/step
Epoch 184/300

Epoch 184: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0714 - Bi-Acc: 0.3920 - val_loss: 0.1044 - val_Bi-Acc: 0.3333 - 15s/epoch - 30ms/step
Epoch 185/300

Epoch 185: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0714 - Bi-Acc: 0.3915 - val_loss: 0.0999 - val_Bi-Acc: 0.3515 - 15s/epoch - 30ms/step
Epoch 186/300

Epoch 186: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0714 - Bi-Acc: 0.3918 - val_loss: 0.1032 - val_Bi-Acc: 0.3394 - 15s/epoch - 30ms/step
Epoch 187/300

Epoch 187: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0714 - Bi-Acc: 0.3915 - val_loss: 0.1018 - val_Bi-Acc: 0.3455 - 15s/epoch - 30ms/step
Epoch 188/300

Epoch 188: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0711 - Bi-Acc: 0.3916 - val_loss: 0.1006 - val_Bi-Acc: 0.3455 - 15s/epoch - 30ms/step
Epoch 189/300

Epoch 189: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0709 - Bi-Acc: 0.3916 - val_loss: 0.1010 - val_Bi-Acc: 0.3455 - 15s/epoch - 30ms/step
Epoch 190/300

Epoch 190: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0709 - Bi-Acc: 0.3919 - val_loss: 0.1053 - val_Bi-Acc: 0.3394 - 15s/epoch - 30ms/step
Epoch 191/300

Epoch 191: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0708 - Bi-Acc: 0.3918 - val_loss: 0.1090 - val_Bi-Acc: 0.3212 - 15s/epoch - 30ms/step
Epoch 192/300

Epoch 192: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0708 - Bi-Acc: 0.3916 - val_loss: 0.1002 - val_Bi-Acc: 0.3515 - 15s/epoch - 30ms/step
Epoch 193/300

Epoch 193: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0708 - Bi-Acc: 0.3919 - val_loss: 0.1032 - val_Bi-Acc: 0.3394 - 15s/epoch - 30ms/step
Epoch 194/300

Epoch 194: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0704 - Bi-Acc: 0.3922 - val_loss: 0.1017 - val_Bi-Acc: 0.3455 - 15s/epoch - 30ms/step
Epoch 195/300

Epoch 195: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0701 - Bi-Acc: 0.3924 - val_loss: 0.1033 - val_Bi-Acc: 0.3394 - 15s/epoch - 30ms/step
Epoch 196/300

Epoch 196: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0701 - Bi-Acc: 0.3925 - val_loss: 0.1041 - val_Bi-Acc: 0.3394 - 15s/epoch - 30ms/step
Epoch 197/300

Epoch 197: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0701 - Bi-Acc: 0.3916 - val_loss: 0.1028 - val_Bi-Acc: 0.3515 - 15s/epoch - 30ms/step
Epoch 198/300

Epoch 198: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0698 - Bi-Acc: 0.3919 - val_loss: 0.1003 - val_Bi-Acc: 0.3515 - 15s/epoch - 30ms/step
Epoch 199/300

Epoch 199: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0700 - Bi-Acc: 0.3923 - val_loss: 0.1019 - val_Bi-Acc: 0.3455 - 15s/epoch - 30ms/step
Epoch 200/300

Epoch 200: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0696 - Bi-Acc: 0.3924 - val_loss: 0.1046 - val_Bi-Acc: 0.3394 - 15s/epoch - 30ms/step
Epoch 201/300

Epoch 201: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0694 - Bi-Acc: 0.3923 - val_loss: 0.1037 - val_Bi-Acc: 0.3394 - 15s/epoch - 30ms/step
Epoch 202/300

Epoch 202: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0695 - Bi-Acc: 0.3925 - val_loss: 0.1044 - val_Bi-Acc: 0.3394 - 15s/epoch - 30ms/step
Epoch 203/300

Epoch 203: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0695 - Bi-Acc: 0.3921 - val_loss: 0.1002 - val_Bi-Acc: 0.3515 - 15s/epoch - 30ms/step
Epoch 204/300

Epoch 204: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0695 - Bi-Acc: 0.3922 - val_loss: 0.1051 - val_Bi-Acc: 0.3333 - 15s/epoch - 30ms/step
Epoch 205/300

Epoch 205: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0693 - Bi-Acc: 0.3926 - val_loss: 0.1023 - val_Bi-Acc: 0.3394 - 15s/epoch - 30ms/step
Epoch 206/300

Epoch 206: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0691 - Bi-Acc: 0.3925 - val_loss: 0.1000 - val_Bi-Acc: 0.3455 - 15s/epoch - 30ms/step
Epoch 207/300

Epoch 207: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0691 - Bi-Acc: 0.3927 - val_loss: 0.1041 - val_Bi-Acc: 0.3515 - 15s/epoch - 30ms/step
Epoch 208/300

Epoch 208: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0688 - Bi-Acc: 0.3928 - val_loss: 0.1060 - val_Bi-Acc: 0.3394 - 15s/epoch - 30ms/step
Epoch 209/300

Epoch 209: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0689 - Bi-Acc: 0.3923 - val_loss: 0.0992 - val_Bi-Acc: 0.3515 - 15s/epoch - 30ms/step
Epoch 210/300

Epoch 210: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0687 - Bi-Acc: 0.3932 - val_loss: 0.1008 - val_Bi-Acc: 0.3515 - 15s/epoch - 30ms/step
Epoch 211/300

Epoch 211: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0688 - Bi-Acc: 0.3930 - val_loss: 0.1012 - val_Bi-Acc: 0.3455 - 15s/epoch - 30ms/step
Epoch 212/300

Epoch 212: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0687 - Bi-Acc: 0.3931 - val_loss: 0.1005 - val_Bi-Acc: 0.3515 - 15s/epoch - 30ms/step
Epoch 213/300

Epoch 213: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0682 - Bi-Acc: 0.3931 - val_loss: 0.1038 - val_Bi-Acc: 0.3515 - 15s/epoch - 30ms/step
Epoch 214/300

Epoch 214: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0685 - Bi-Acc: 0.3924 - val_loss: 0.1049 - val_Bi-Acc: 0.3394 - 15s/epoch - 30ms/step
Epoch 215/300

Epoch 215: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0684 - Bi-Acc: 0.3928 - val_loss: 0.1004 - val_Bi-Acc: 0.3636 - 15s/epoch - 30ms/step
Epoch 216/300

Epoch 216: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0683 - Bi-Acc: 0.3925 - val_loss: 0.1032 - val_Bi-Acc: 0.3576 - 15s/epoch - 30ms/step
Epoch 217/300

Epoch 217: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0681 - Bi-Acc: 0.3931 - val_loss: 0.1125 - val_Bi-Acc: 0.3273 - 15s/epoch - 30ms/step
Epoch 218/300

Epoch 218: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0679 - Bi-Acc: 0.3931 - val_loss: 0.1015 - val_Bi-Acc: 0.3576 - 15s/epoch - 30ms/step
Epoch 219/300

Epoch 219: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0681 - Bi-Acc: 0.3931 - val_loss: 0.1104 - val_Bi-Acc: 0.3333 - 15s/epoch - 30ms/step
Epoch 220/300

Epoch 220: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0676 - Bi-Acc: 0.3935 - val_loss: 0.1020 - val_Bi-Acc: 0.3576 - 15s/epoch - 30ms/step
Epoch 221/300

Epoch 221: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0679 - Bi-Acc: 0.3935 - val_loss: 0.1043 - val_Bi-Acc: 0.3515 - 15s/epoch - 30ms/step
Epoch 222/300

Epoch 222: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0678 - Bi-Acc: 0.3934 - val_loss: 0.1070 - val_Bi-Acc: 0.3333 - 15s/epoch - 30ms/step
Epoch 223/300

Epoch 223: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0677 - Bi-Acc: 0.3931 - val_loss: 0.1038 - val_Bi-Acc: 0.3515 - 15s/epoch - 30ms/step
Epoch 224/300

Epoch 224: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0671 - Bi-Acc: 0.3942 - val_loss: 0.1046 - val_Bi-Acc: 0.3515 - 15s/epoch - 30ms/step
Epoch 225/300

Epoch 225: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0674 - Bi-Acc: 0.3934 - val_loss: 0.1068 - val_Bi-Acc: 0.3455 - 15s/epoch - 30ms/step
Epoch 226/300

Epoch 226: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0675 - Bi-Acc: 0.3936 - val_loss: 0.1019 - val_Bi-Acc: 0.3576 - 15s/epoch - 30ms/step
Epoch 227/300

Epoch 227: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0673 - Bi-Acc: 0.3932 - val_loss: 0.1075 - val_Bi-Acc: 0.3394 - 15s/epoch - 30ms/step
Epoch 228/300

Epoch 228: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0673 - Bi-Acc: 0.3930 - val_loss: 0.1023 - val_Bi-Acc: 0.3515 - 15s/epoch - 30ms/step
Epoch 229/300

Epoch 229: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0672 - Bi-Acc: 0.3938 - val_loss: 0.1054 - val_Bi-Acc: 0.3455 - 15s/epoch - 30ms/step
Epoch 230/300

Epoch 230: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0673 - Bi-Acc: 0.3935 - val_loss: 0.1068 - val_Bi-Acc: 0.3455 - 15s/epoch - 30ms/step
Epoch 231/300

Epoch 231: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0669 - Bi-Acc: 0.3938 - val_loss: 0.1055 - val_Bi-Acc: 0.3455 - 15s/epoch - 30ms/step
Epoch 232/300

Epoch 232: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0669 - Bi-Acc: 0.3940 - val_loss: 0.1044 - val_Bi-Acc: 0.3455 - 15s/epoch - 30ms/step
Epoch 233/300

Epoch 233: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0670 - Bi-Acc: 0.3937 - val_loss: 0.1037 - val_Bi-Acc: 0.3515 - 15s/epoch - 30ms/step
Epoch 234/300

Epoch 234: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0666 - Bi-Acc: 0.3940 - val_loss: 0.1068 - val_Bi-Acc: 0.3394 - 15s/epoch - 30ms/step
Epoch 235/300

Epoch 235: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0666 - Bi-Acc: 0.3938 - val_loss: 0.1022 - val_Bi-Acc: 0.3515 - 15s/epoch - 30ms/step
Epoch 236/300

Epoch 236: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0665 - Bi-Acc: 0.3940 - val_loss: 0.1053 - val_Bi-Acc: 0.3333 - 15s/epoch - 30ms/step
Epoch 237/300

Epoch 237: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0664 - Bi-Acc: 0.3937 - val_loss: 0.1089 - val_Bi-Acc: 0.3273 - 15s/epoch - 30ms/step
Epoch 238/300

Epoch 238: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0661 - Bi-Acc: 0.3942 - val_loss: 0.1071 - val_Bi-Acc: 0.3333 - 15s/epoch - 30ms/step
Epoch 239/300

Epoch 239: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0663 - Bi-Acc: 0.3941 - val_loss: 0.1061 - val_Bi-Acc: 0.3515 - 15s/epoch - 30ms/step
Epoch 240/300

Epoch 240: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0664 - Bi-Acc: 0.3940 - val_loss: 0.1030 - val_Bi-Acc: 0.3515 - 15s/epoch - 30ms/step
Epoch 241/300

Epoch 241: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0661 - Bi-Acc: 0.3942 - val_loss: 0.1027 - val_Bi-Acc: 0.3515 - 15s/epoch - 30ms/step
Epoch 242/300

Epoch 242: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0660 - Bi-Acc: 0.3944 - val_loss: 0.1027 - val_Bi-Acc: 0.3455 - 15s/epoch - 30ms/step
Epoch 243/300

Epoch 243: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0660 - Bi-Acc: 0.3938 - val_loss: 0.1057 - val_Bi-Acc: 0.3333 - 15s/epoch - 30ms/step
Epoch 244/300

Epoch 244: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0659 - Bi-Acc: 0.3941 - val_loss: 0.1010 - val_Bi-Acc: 0.3576 - 15s/epoch - 30ms/step
Epoch 245/300

Epoch 245: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0659 - Bi-Acc: 0.3940 - val_loss: 0.1032 - val_Bi-Acc: 0.3455 - 15s/epoch - 30ms/step
Epoch 246/300

Epoch 246: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0658 - Bi-Acc: 0.3940 - val_loss: 0.1052 - val_Bi-Acc: 0.3394 - 15s/epoch - 30ms/step
Epoch 247/300

Epoch 247: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0658 - Bi-Acc: 0.3943 - val_loss: 0.1050 - val_Bi-Acc: 0.3455 - 15s/epoch - 30ms/step
Epoch 248/300

Epoch 248: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0656 - Bi-Acc: 0.3945 - val_loss: 0.1067 - val_Bi-Acc: 0.3394 - 15s/epoch - 30ms/step
Epoch 249/300

Epoch 249: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0656 - Bi-Acc: 0.3943 - val_loss: 0.1065 - val_Bi-Acc: 0.3394 - 15s/epoch - 30ms/step
Epoch 250/300

Epoch 250: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0655 - Bi-Acc: 0.3939 - val_loss: 0.1029 - val_Bi-Acc: 0.3455 - 15s/epoch - 30ms/step
Epoch 251/300

Epoch 251: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0653 - Bi-Acc: 0.3944 - val_loss: 0.1027 - val_Bi-Acc: 0.3455 - 15s/epoch - 30ms/step
Epoch 252/300

Epoch 252: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0653 - Bi-Acc: 0.3941 - val_loss: 0.1059 - val_Bi-Acc: 0.3333 - 15s/epoch - 30ms/step
Epoch 253/300

Epoch 253: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0655 - Bi-Acc: 0.3942 - val_loss: 0.1052 - val_Bi-Acc: 0.3455 - 15s/epoch - 30ms/step
Epoch 254/300

Epoch 254: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0651 - Bi-Acc: 0.3945 - val_loss: 0.1013 - val_Bi-Acc: 0.3515 - 15s/epoch - 30ms/step
Epoch 255/300

Epoch 255: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0651 - Bi-Acc: 0.3942 - val_loss: 0.1014 - val_Bi-Acc: 0.3455 - 15s/epoch - 30ms/step
Epoch 256/300

Epoch 256: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0652 - Bi-Acc: 0.3943 - val_loss: 0.1030 - val_Bi-Acc: 0.3394 - 15s/epoch - 30ms/step
Epoch 257/300

Epoch 257: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0650 - Bi-Acc: 0.3945 - val_loss: 0.1043 - val_Bi-Acc: 0.3394 - 15s/epoch - 30ms/step
Epoch 258/300

Epoch 258: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0651 - Bi-Acc: 0.3944 - val_loss: 0.1045 - val_Bi-Acc: 0.3515 - 15s/epoch - 30ms/step
Epoch 259/300

Epoch 259: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0648 - Bi-Acc: 0.3948 - val_loss: 0.1035 - val_Bi-Acc: 0.3455 - 15s/epoch - 30ms/step
Epoch 260/300

Epoch 260: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0648 - Bi-Acc: 0.3948 - val_loss: 0.1083 - val_Bi-Acc: 0.3394 - 15s/epoch - 30ms/step
Epoch 261/300

Epoch 261: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0647 - Bi-Acc: 0.3946 - val_loss: 0.1075 - val_Bi-Acc: 0.3212 - 15s/epoch - 30ms/step
Epoch 262/300

Epoch 262: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0645 - Bi-Acc: 0.3948 - val_loss: 0.1005 - val_Bi-Acc: 0.3515 - 15s/epoch - 30ms/step
Epoch 263/300

Epoch 263: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0646 - Bi-Acc: 0.3949 - val_loss: 0.1038 - val_Bi-Acc: 0.3576 - 15s/epoch - 30ms/step
Epoch 264/300

Epoch 264: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0645 - Bi-Acc: 0.3951 - val_loss: 0.1031 - val_Bi-Acc: 0.3455 - 15s/epoch - 30ms/step
Epoch 265/300

Epoch 265: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0645 - Bi-Acc: 0.3949 - val_loss: 0.1018 - val_Bi-Acc: 0.3455 - 15s/epoch - 30ms/step
Epoch 266/300

Epoch 266: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0645 - Bi-Acc: 0.3948 - val_loss: 0.1051 - val_Bi-Acc: 0.3273 - 15s/epoch - 30ms/step
Epoch 267/300

Epoch 267: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0644 - Bi-Acc: 0.3947 - val_loss: 0.1042 - val_Bi-Acc: 0.3394 - 15s/epoch - 30ms/step
Epoch 268/300

Epoch 268: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0643 - Bi-Acc: 0.3947 - val_loss: 0.1046 - val_Bi-Acc: 0.3394 - 15s/epoch - 30ms/step
Epoch 269/300

Epoch 269: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0642 - Bi-Acc: 0.3948 - val_loss: 0.1032 - val_Bi-Acc: 0.3333 - 15s/epoch - 30ms/step
Epoch 270/300

Epoch 270: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0641 - Bi-Acc: 0.3952 - val_loss: 0.1077 - val_Bi-Acc: 0.3394 - 15s/epoch - 30ms/step
Epoch 271/300

Epoch 271: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0642 - Bi-Acc: 0.3949 - val_loss: 0.1016 - val_Bi-Acc: 0.3576 - 15s/epoch - 30ms/step
Epoch 272/300

Epoch 272: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0640 - Bi-Acc: 0.3952 - val_loss: 0.1039 - val_Bi-Acc: 0.3515 - 15s/epoch - 30ms/step
Epoch 273/300

Epoch 273: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0640 - Bi-Acc: 0.3948 - val_loss: 0.1100 - val_Bi-Acc: 0.3333 - 15s/epoch - 30ms/step
Epoch 274/300

Epoch 274: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0643 - Bi-Acc: 0.3947 - val_loss: 0.1059 - val_Bi-Acc: 0.3333 - 15s/epoch - 30ms/step
Epoch 275/300

Epoch 275: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0640 - Bi-Acc: 0.3947 - val_loss: 0.1074 - val_Bi-Acc: 0.3333 - 15s/epoch - 30ms/step
Epoch 276/300

Epoch 276: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0637 - Bi-Acc: 0.3954 - val_loss: 0.1061 - val_Bi-Acc: 0.3333 - 15s/epoch - 30ms/step
Epoch 277/300

Epoch 277: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0637 - Bi-Acc: 0.3947 - val_loss: 0.1060 - val_Bi-Acc: 0.3394 - 15s/epoch - 30ms/step
Epoch 278/300

Epoch 278: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0636 - Bi-Acc: 0.3946 - val_loss: 0.1067 - val_Bi-Acc: 0.3333 - 15s/epoch - 30ms/step
Epoch 279/300

Epoch 279: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0636 - Bi-Acc: 0.3953 - val_loss: 0.1081 - val_Bi-Acc: 0.3333 - 15s/epoch - 30ms/step
Epoch 280/300

Epoch 280: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0638 - Bi-Acc: 0.3947 - val_loss: 0.1059 - val_Bi-Acc: 0.3333 - 15s/epoch - 30ms/step
Epoch 281/300

Epoch 281: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0635 - Bi-Acc: 0.3951 - val_loss: 0.1094 - val_Bi-Acc: 0.3273 - 15s/epoch - 30ms/step
Epoch 282/300

Epoch 282: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0637 - Bi-Acc: 0.3951 - val_loss: 0.1113 - val_Bi-Acc: 0.3273 - 15s/epoch - 30ms/step
Epoch 283/300

Epoch 283: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0635 - Bi-Acc: 0.3951 - val_loss: 0.1082 - val_Bi-Acc: 0.3273 - 15s/epoch - 30ms/step
Epoch 284/300

Epoch 284: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0631 - Bi-Acc: 0.3956 - val_loss: 0.1073 - val_Bi-Acc: 0.3394 - 15s/epoch - 30ms/step
Epoch 285/300

Epoch 285: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0635 - Bi-Acc: 0.3948 - val_loss: 0.1014 - val_Bi-Acc: 0.3515 - 15s/epoch - 30ms/step
Epoch 286/300

Epoch 286: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0634 - Bi-Acc: 0.3948 - val_loss: 0.1096 - val_Bi-Acc: 0.3394 - 15s/epoch - 30ms/step
Epoch 287/300

Epoch 287: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0634 - Bi-Acc: 0.3949 - val_loss: 0.1105 - val_Bi-Acc: 0.3333 - 15s/epoch - 30ms/step
Epoch 288/300

Epoch 288: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0632 - Bi-Acc: 0.3952 - val_loss: 0.1060 - val_Bi-Acc: 0.3394 - 15s/epoch - 30ms/step
Epoch 289/300

Epoch 289: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0632 - Bi-Acc: 0.3953 - val_loss: 0.1023 - val_Bi-Acc: 0.3455 - 15s/epoch - 30ms/step
Epoch 290/300

Epoch 290: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0632 - Bi-Acc: 0.3952 - val_loss: 0.1087 - val_Bi-Acc: 0.3394 - 15s/epoch - 30ms/step
Epoch 291/300

Epoch 291: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0631 - Bi-Acc: 0.3953 - val_loss: 0.1093 - val_Bi-Acc: 0.3273 - 15s/epoch - 30ms/step
Epoch 292/300

Epoch 292: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0628 - Bi-Acc: 0.3956 - val_loss: 0.1078 - val_Bi-Acc: 0.3394 - 15s/epoch - 30ms/step
Epoch 293/300

Epoch 293: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0629 - Bi-Acc: 0.3953 - val_loss: 0.1108 - val_Bi-Acc: 0.3212 - 15s/epoch - 30ms/step
Epoch 294/300

Epoch 294: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0628 - Bi-Acc: 0.3953 - val_loss: 0.1045 - val_Bi-Acc: 0.3455 - 15s/epoch - 30ms/step
Epoch 295/300

Epoch 295: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0626 - Bi-Acc: 0.3954 - val_loss: 0.1047 - val_Bi-Acc: 0.3333 - 15s/epoch - 30ms/step
Epoch 296/300

Epoch 296: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0628 - Bi-Acc: 0.3957 - val_loss: 0.1112 - val_Bi-Acc: 0.3273 - 15s/epoch - 30ms/step
Epoch 297/300

Epoch 297: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0628 - Bi-Acc: 0.3950 - val_loss: 0.1073 - val_Bi-Acc: 0.3333 - 15s/epoch - 30ms/step
Epoch 298/300

Epoch 298: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0626 - Bi-Acc: 0.3955 - val_loss: 0.1077 - val_Bi-Acc: 0.3333 - 15s/epoch - 30ms/step
Epoch 299/300

Epoch 299: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0627 - Bi-Acc: 0.3953 - val_loss: 0.1067 - val_Bi-Acc: 0.3455 - 15s/epoch - 30ms/step
Epoch 300/300

Epoch 300: val_loss did not improve from 0.09723
505/505 - 15s - loss: 0.0626 - Bi-Acc: 0.3956 - val_loss: 0.1062 - val_Bi-Acc: 0.3333 - 15s/epoch - 30ms/step
6/6 - 0s - 211ms/epoch - 35ms/step
Validation:

Confusion Matrix for 0
True Pos False Neg
[72 12]
False Pos True Neg
[13 68]
Precision: 0.8470588235294118
Recall: 0.8571428571428571
F1 Score for Neg: 0.84472049689441
F1 Score for Pos: 0.8520710059171598
4/4 - 0s - 65ms/epoch - 16ms/step
Test:

Confusion Matrix for 0
True Pos False Neg
[53 10]
False Pos True Neg
[16 42]
Precision: 0.7681159420289855
Recall: 0.8412698412698413
F1 Score for Neg: 0.7636363636363636
F1 Score for Pos: 0.8030303030303031

Saved
