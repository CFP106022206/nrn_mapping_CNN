2024-01-21 06:11:52.006954: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-01-21 06:11:52.072341: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-01-21 06:11:52.073229: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-01-21 06:11:52.074447: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-01-21 06:11:52.079741: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-01-21 06:11:54.572895: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-01-21 06:12:05.924179: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-01-21 06:12:05.989086: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-01-21 06:12:05.989409: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-01-21 06:12:05.990178: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-01-21 06:12:05.990346: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-01-21 06:12:05.990487: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-01-21 06:12:06.071256: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-01-21 06:12:06.071495: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-01-21 06:12:06.072209: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-01-21 06:12:06.072302: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22461 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:00:10.0, compute capability: 8.6

Collecting 3-View Data Numpy Array..

 Resolutions: (3, 50, 50)

Collecting 3-View Data Numpy Array..

 Resolutions: (3, 50, 50)

Train data: 933 
Valid data: 165 
Test data: 121
Total: 933
Positive: 428 (45.87% of total)

Balanced Weight in: 
 [0 1] 
 [0.92376238 1.08995327]
UpSampling: After Augmentation:
True Label/Total in X_train:
 428 / 1010
X_train shape: (32320, 50, 50, 3) (32320, 50, 50, 3)
y_train shape: 32320
X_val shape: (165, 50, 50, 3) (165, 50, 50, 3)
y_val shape: 165
X_test shape: (121, 50, 50, 3) (121, 50, 50, 3)
y_test shape: 121
Model: "model"
__________________________________________________________________________________________________
 Layer (type)                Output Shape                 Param #   Connected to                  
==================================================================================================
 FC (InputLayer)             [(None, 50, 50, 3)]          0         []                            
                                                                                                  
 EM (InputLayer)             [(None, 50, 50, 3)]          0         []                            
                                                                                                  
 conv1 (Conv2D)              (None, 50, 50, 32)           896       ['FC[0][0]',                  
                                                                     'EM[0][0]']                  
                                                                                                  
 bn1 (BatchNormalization)    (None, 50, 50, 32)           128       ['conv1[0][0]',               
                                                                     'conv1[1][0]']               
                                                                                                  
 ac1 (Activation)            (None, 50, 50, 32)           0         ['bn1[0][0]',                 
                                                                     'bn1[1][0]']                 
                                                                                                  
 conv2 (Conv2D)              (None, 50, 50, 32)           9248      ['ac1[0][0]',                 
                                                                     'ac1[1][0]']                 
                                                                                                  
 bn2 (BatchNormalization)    (None, 50, 50, 32)           128       ['conv2[0][0]',               
                                                                     'conv2[1][0]']               
                                                                                                  
 ac2 (Activation)            (None, 50, 50, 32)           0         ['bn2[0][0]',                 
                                                                     'bn2[1][0]']                 
                                                                                                  
 pool1 (MaxPooling2D)        (None, 25, 25, 32)           0         ['ac2[0][0]',                 
                                                                     'ac2[1][0]']                 
                                                                                                  
 conv3 (Conv2D)              (None, 25, 25, 64)           18496     ['pool1[0][0]',               
                                                                     'pool1[1][0]']               
                                                                                                  
 bn3 (BatchNormalization)    (None, 25, 25, 64)           256       ['conv3[0][0]',               
                                                                     'conv3[1][0]']               
                                                                                                  
 ac3 (Activation)            (None, 25, 25, 64)           0         ['bn3[0][0]',                 
                                                                     'bn3[1][0]']                 
                                                                                                  
 conv4 (Conv2D)              (None, 25, 25, 64)           36928     ['ac3[0][0]',                 
                                                                     'ac3[1][0]']                 
                                                                                                  
 bn4 (BatchNormalization)    (None, 25, 25, 64)           256       ['conv4[0][0]',               
                                                                     'conv4[1][0]']               
                                                                                                  
 ac4 (Activation)            (None, 25, 25, 64)           0         ['bn4[0][0]',                 
                                                                     'bn4[1][0]']                 
                                                                                                  
 pool2 (MaxPooling2D)        (None, 12, 12, 64)           0         ['ac4[0][0]',                 
                                                                     'ac4[1][0]']                 
                                                                                                  
 dropout (Dropout)           (None, 12, 12, 64)           0         ['pool2[0][0]']               
                                                                                                  
 dropout_1 (Dropout)         (None, 12, 12, 64)           0         ['pool2[1][0]']               
                                                                                                  
 flatten (Flatten)           (None, 9216)                 0         ['dropout[0][0]']             
                                                                                                  
 flatten_1 (Flatten)         (None, 9216)                 0         ['dropout_1[0][0]']           
                                                                                                  
 concatenate (Concatenate)   (None, 18432)                0         ['flatten[0][0]',             
                                                                     'flatten_1[0][0]']           
                                                                                                  
 dropout_2 (Dropout)         (None, 18432)                0         ['concatenate[0][0]']         
                                                                                                  
 dense (Dense)               (None, 128)                  2359424   ['dropout_2[0][0]']           
                                                                                                  
 batch_normalization (Batch  (None, 128)                  512       ['dense[0][0]']               
 Normalization)                                                                                   
                                                                                                  
 activation (Activation)     (None, 128)                  0         ['batch_normalization[0][0]'] 
                                                                                                  
 dense_1 (Dense)             (None, 1)                    129       ['activation[0][0]']          
                                                                                                  
==================================================================================================
Total params: 2426401 (9.26 MB)
Trainable params: 2425761 (9.25 MB)
Non-trainable params: 640 (2.50 KB)
__________________________________________________________________________________________________

Use Callbacks: [<keras.src.callbacks.ModelCheckpoint object at 0x7fafdc23cb50>]
Epoch 1/300
2024-01-21 06:12:14.017634: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:961] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape inmodel/dropout_1/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer
2024-01-21 06:12:14.505349: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8904
2024-01-21 06:12:14.634989: I external/local_tsl/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory
2024-01-21 06:12:14.791218: I external/local_tsl/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory
2024-01-21 06:12:17.050856: I external/local_xla/xla/service/service.cc:168] XLA service 0x7fae45a93f60 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2024-01-21 06:12:17.050992: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 3090, Compute Capability 8.6
2024-01-21 06:12:17.057493: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1705788737.194686  289554 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.

Epoch 1: val_loss improved from inf to 0.16220, saving model to ./Annotator_Model/Annotator_D1-D6_9.h5
/cluster/home/ming/anaconda3/envs/ming/lib/python3.9/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.
  saving_api.save_model(
253/253 - 17s - loss: 0.2298 - Bi-Acc: 0.2818 - val_loss: 0.1622 - val_Bi-Acc: 0.3273 - 17s/epoch - 69ms/step
Epoch 2/300

Epoch 2: val_loss improved from 0.16220 to 0.16009, saving model to ./Annotator_Model/Annotator_D1-D6_9.h5
253/253 - 10s - loss: 0.2102 - Bi-Acc: 0.2889 - val_loss: 0.1601 - val_Bi-Acc: 0.2182 - 10s/epoch - 40ms/step
Epoch 3/300

Epoch 3: val_loss did not improve from 0.16009
253/253 - 10s - loss: 0.2015 - Bi-Acc: 0.2925 - val_loss: 0.1672 - val_Bi-Acc: 0.1939 - 10s/epoch - 39ms/step
Epoch 4/300

Epoch 4: val_loss did not improve from 0.16009
253/253 - 9s - loss: 0.1926 - Bi-Acc: 0.2936 - val_loss: 0.1755 - val_Bi-Acc: 0.1818 - 9s/epoch - 35ms/step
Epoch 5/300

Epoch 5: val_loss did not improve from 0.16009
253/253 - 9s - loss: 0.1858 - Bi-Acc: 0.2975 - val_loss: 0.1688 - val_Bi-Acc: 0.1758 - 9s/epoch - 37ms/step
Epoch 6/300

Epoch 6: val_loss did not improve from 0.16009
253/253 - 10s - loss: 0.1794 - Bi-Acc: 0.2985 - val_loss: 0.1679 - val_Bi-Acc: 0.1818 - 10s/epoch - 38ms/step
Epoch 7/300

Epoch 7: val_loss did not improve from 0.16009
253/253 - 9s - loss: 0.1753 - Bi-Acc: 0.3003 - val_loss: 0.1727 - val_Bi-Acc: 0.1818 - 9s/epoch - 36ms/step
Epoch 8/300

Epoch 8: val_loss did not improve from 0.16009
253/253 - 9s - loss: 0.1706 - Bi-Acc: 0.3024 - val_loss: 0.1649 - val_Bi-Acc: 0.1879 - 9s/epoch - 36ms/step
Epoch 9/300

Epoch 9: val_loss did not improve from 0.16009
253/253 - 10s - loss: 0.1675 - Bi-Acc: 0.3039 - val_loss: 0.1691 - val_Bi-Acc: 0.1697 - 10s/epoch - 38ms/step
Epoch 10/300

Epoch 10: val_loss did not improve from 0.16009
253/253 - 9s - loss: 0.1632 - Bi-Acc: 0.3065 - val_loss: 0.1619 - val_Bi-Acc: 0.1879 - 9s/epoch - 36ms/step
Epoch 11/300

Epoch 11: val_loss did not improve from 0.16009
253/253 - 9s - loss: 0.1611 - Bi-Acc: 0.3109 - val_loss: 0.1684 - val_Bi-Acc: 0.1697 - 9s/epoch - 36ms/step
Epoch 12/300

Epoch 12: val_loss improved from 0.16009 to 0.15981, saving model to ./Annotator_Model/Annotator_D1-D6_9.h5
253/253 - 10s - loss: 0.1583 - Bi-Acc: 0.3132 - val_loss: 0.1598 - val_Bi-Acc: 0.1939 - 10s/epoch - 39ms/step
Epoch 13/300

Epoch 13: val_loss did not improve from 0.15981
253/253 - 10s - loss: 0.1553 - Bi-Acc: 0.3150 - val_loss: 0.1606 - val_Bi-Acc: 0.1939 - 10s/epoch - 38ms/step
Epoch 14/300

Epoch 14: val_loss improved from 0.15981 to 0.15777, saving model to ./Annotator_Model/Annotator_D1-D6_9.h5
253/253 - 9s - loss: 0.1539 - Bi-Acc: 0.3166 - val_loss: 0.1578 - val_Bi-Acc: 0.2121 - 9s/epoch - 37ms/step
Epoch 15/300

Epoch 15: val_loss did not improve from 0.15777
253/253 - 10s - loss: 0.1505 - Bi-Acc: 0.3216 - val_loss: 0.1640 - val_Bi-Acc: 0.1939 - 10s/epoch - 38ms/step
Epoch 16/300

Epoch 16: val_loss improved from 0.15777 to 0.15460, saving model to ./Annotator_Model/Annotator_D1-D6_9.h5
253/253 - 10s - loss: 0.1495 - Bi-Acc: 0.3231 - val_loss: 0.1546 - val_Bi-Acc: 0.2121 - 10s/epoch - 39ms/step
Epoch 17/300

Epoch 17: val_loss improved from 0.15460 to 0.14890, saving model to ./Annotator_Model/Annotator_D1-D6_9.h5
253/253 - 10s - loss: 0.1484 - Bi-Acc: 0.3225 - val_loss: 0.1489 - val_Bi-Acc: 0.2121 - 10s/epoch - 38ms/step
Epoch 18/300

Epoch 18: val_loss improved from 0.14890 to 0.14635, saving model to ./Annotator_Model/Annotator_D1-D6_9.h5
253/253 - 10s - loss: 0.1458 - Bi-Acc: 0.3254 - val_loss: 0.1464 - val_Bi-Acc: 0.2364 - 10s/epoch - 40ms/step
Epoch 19/300

Epoch 19: val_loss did not improve from 0.14635
253/253 - 9s - loss: 0.1439 - Bi-Acc: 0.3263 - val_loss: 0.1479 - val_Bi-Acc: 0.2182 - 9s/epoch - 36ms/step
Epoch 20/300

Epoch 20: val_loss improved from 0.14635 to 0.14515, saving model to ./Annotator_Model/Annotator_D1-D6_9.h5
253/253 - 9s - loss: 0.1430 - Bi-Acc: 0.3302 - val_loss: 0.1451 - val_Bi-Acc: 0.2424 - 9s/epoch - 37ms/step
Epoch 21/300

Epoch 21: val_loss did not improve from 0.14515
253/253 - 10s - loss: 0.1421 - Bi-Acc: 0.3287 - val_loss: 0.1502 - val_Bi-Acc: 0.2242 - 10s/epoch - 38ms/step
Epoch 22/300

Epoch 22: val_loss improved from 0.14515 to 0.14370, saving model to ./Annotator_Model/Annotator_D1-D6_9.h5
253/253 - 10s - loss: 0.1396 - Bi-Acc: 0.3331 - val_loss: 0.1437 - val_Bi-Acc: 0.2485 - 10s/epoch - 40ms/step
Epoch 23/300

Epoch 23: val_loss improved from 0.14370 to 0.14352, saving model to ./Annotator_Model/Annotator_D1-D6_9.h5
253/253 - 9s - loss: 0.1382 - Bi-Acc: 0.3346 - val_loss: 0.1435 - val_Bi-Acc: 0.2485 - 9s/epoch - 36ms/step
Epoch 24/300

Epoch 24: val_loss did not improve from 0.14352
253/253 - 10s - loss: 0.1375 - Bi-Acc: 0.3341 - val_loss: 0.1467 - val_Bi-Acc: 0.2364 - 10s/epoch - 38ms/step
Epoch 25/300

Epoch 25: val_loss did not improve from 0.14352
253/253 - 9s - loss: 0.1351 - Bi-Acc: 0.3378 - val_loss: 0.1479 - val_Bi-Acc: 0.2424 - 9s/epoch - 38ms/step
Epoch 26/300

Epoch 26: val_loss improved from 0.14352 to 0.14203, saving model to ./Annotator_Model/Annotator_D1-D6_9.h5
253/253 - 9s - loss: 0.1342 - Bi-Acc: 0.3382 - val_loss: 0.1420 - val_Bi-Acc: 0.2485 - 9s/epoch - 37ms/step
Epoch 27/300

Epoch 27: val_loss improved from 0.14203 to 0.13379, saving model to ./Annotator_Model/Annotator_D1-D6_9.h5
253/253 - 10s - loss: 0.1330 - Bi-Acc: 0.3390 - val_loss: 0.1338 - val_Bi-Acc: 0.2667 - 10s/epoch - 40ms/step
Epoch 28/300

Epoch 28: val_loss did not improve from 0.13379
253/253 - 9s - loss: 0.1326 - Bi-Acc: 0.3412 - val_loss: 0.1432 - val_Bi-Acc: 0.2424 - 9s/epoch - 36ms/step
Epoch 29/300

Epoch 29: val_loss did not improve from 0.13379
253/253 - 9s - loss: 0.1307 - Bi-Acc: 0.3403 - val_loss: 0.1446 - val_Bi-Acc: 0.2364 - 9s/epoch - 35ms/step
Epoch 30/300

Epoch 30: val_loss did not improve from 0.13379
253/253 - 9s - loss: 0.1292 - Bi-Acc: 0.3439 - val_loss: 0.1345 - val_Bi-Acc: 0.2606 - 9s/epoch - 37ms/step
Epoch 31/300

Epoch 31: val_loss did not improve from 0.13379
253/253 - 10s - loss: 0.1283 - Bi-Acc: 0.3441 - val_loss: 0.1370 - val_Bi-Acc: 0.2424 - 10s/epoch - 38ms/step
Epoch 32/300

Epoch 32: val_loss improved from 0.13379 to 0.13238, saving model to ./Annotator_Model/Annotator_D1-D6_9.h5
253/253 - 9s - loss: 0.1279 - Bi-Acc: 0.3452 - val_loss: 0.1324 - val_Bi-Acc: 0.2727 - 9s/epoch - 37ms/step
Epoch 33/300

Epoch 33: val_loss did not improve from 0.13238
253/253 - 9s - loss: 0.1263 - Bi-Acc: 0.3458 - val_loss: 0.1453 - val_Bi-Acc: 0.2364 - 9s/epoch - 37ms/step
Epoch 34/300

Epoch 34: val_loss did not improve from 0.13238
253/253 - 10s - loss: 0.1273 - Bi-Acc: 0.3446 - val_loss: 0.1401 - val_Bi-Acc: 0.2485 - 10s/epoch - 38ms/step
Epoch 35/300

Epoch 35: val_loss did not improve from 0.13238
253/253 - 9s - loss: 0.1251 - Bi-Acc: 0.3458 - val_loss: 0.1389 - val_Bi-Acc: 0.2485 - 9s/epoch - 35ms/step
Epoch 36/300

Epoch 36: val_loss did not improve from 0.13238
253/253 - 9s - loss: 0.1242 - Bi-Acc: 0.3494 - val_loss: 0.1405 - val_Bi-Acc: 0.2485 - 9s/epoch - 37ms/step
Epoch 37/300

Epoch 37: val_loss did not improve from 0.13238
253/253 - 10s - loss: 0.1231 - Bi-Acc: 0.3487 - val_loss: 0.1347 - val_Bi-Acc: 0.2606 - 10s/epoch - 38ms/step
Epoch 38/300

Epoch 38: val_loss did not improve from 0.13238
253/253 - 9s - loss: 0.1229 - Bi-Acc: 0.3481 - val_loss: 0.1329 - val_Bi-Acc: 0.2667 - 9s/epoch - 35ms/step
Epoch 39/300

Epoch 39: val_loss improved from 0.13238 to 0.12697, saving model to ./Annotator_Model/Annotator_D1-D6_9.h5
253/253 - 10s - loss: 0.1223 - Bi-Acc: 0.3485 - val_loss: 0.1270 - val_Bi-Acc: 0.2727 - 10s/epoch - 38ms/step
Epoch 40/300

Epoch 40: val_loss did not improve from 0.12697
253/253 - 10s - loss: 0.1204 - Bi-Acc: 0.3514 - val_loss: 0.1274 - val_Bi-Acc: 0.2667 - 10s/epoch - 39ms/step
Epoch 41/300

Epoch 41: val_loss did not improve from 0.12697
253/253 - 9s - loss: 0.1193 - Bi-Acc: 0.3537 - val_loss: 0.1314 - val_Bi-Acc: 0.2606 - 9s/epoch - 36ms/step
Epoch 42/300

Epoch 42: val_loss improved from 0.12697 to 0.12503, saving model to ./Annotator_Model/Annotator_D1-D6_9.h5
253/253 - 9s - loss: 0.1196 - Bi-Acc: 0.3526 - val_loss: 0.1250 - val_Bi-Acc: 0.2727 - 9s/epoch - 37ms/step
Epoch 43/300

Epoch 43: val_loss did not improve from 0.12503
253/253 - 10s - loss: 0.1188 - Bi-Acc: 0.3537 - val_loss: 0.1257 - val_Bi-Acc: 0.2727 - 10s/epoch - 38ms/step
Epoch 44/300

Epoch 44: val_loss improved from 0.12503 to 0.12297, saving model to ./Annotator_Model/Annotator_D1-D6_9.h5
253/253 - 10s - loss: 0.1178 - Bi-Acc: 0.3535 - val_loss: 0.1230 - val_Bi-Acc: 0.2727 - 10s/epoch - 38ms/step
Epoch 45/300

Epoch 45: val_loss did not improve from 0.12297
253/253 - 9s - loss: 0.1170 - Bi-Acc: 0.3563 - val_loss: 0.1361 - val_Bi-Acc: 0.2606 - 9s/epoch - 35ms/step
Epoch 46/300

Epoch 46: val_loss did not improve from 0.12297
253/253 - 10s - loss: 0.1158 - Bi-Acc: 0.3555 - val_loss: 0.1242 - val_Bi-Acc: 0.2667 - 10s/epoch - 38ms/step
Epoch 47/300

Epoch 47: val_loss did not improve from 0.12297
253/253 - 9s - loss: 0.1162 - Bi-Acc: 0.3556 - val_loss: 0.1295 - val_Bi-Acc: 0.2606 - 9s/epoch - 37ms/step
Epoch 48/300

Epoch 48: val_loss improved from 0.12297 to 0.12092, saving model to ./Annotator_Model/Annotator_D1-D6_9.h5
253/253 - 9s - loss: 0.1154 - Bi-Acc: 0.3562 - val_loss: 0.1209 - val_Bi-Acc: 0.2970 - 9s/epoch - 37ms/step
Epoch 49/300

Epoch 49: val_loss improved from 0.12092 to 0.12064, saving model to ./Annotator_Model/Annotator_D1-D6_9.h5
253/253 - 10s - loss: 0.1144 - Bi-Acc: 0.3592 - val_loss: 0.1206 - val_Bi-Acc: 0.2970 - 10s/epoch - 39ms/step
Epoch 50/300

Epoch 50: val_loss did not improve from 0.12064
253/253 - 10s - loss: 0.1136 - Bi-Acc: 0.3581 - val_loss: 0.1211 - val_Bi-Acc: 0.2909 - 10s/epoch - 38ms/step
Epoch 51/300

Epoch 51: val_loss did not improve from 0.12064
253/253 - 9s - loss: 0.1134 - Bi-Acc: 0.3586 - val_loss: 0.1212 - val_Bi-Acc: 0.2848 - 9s/epoch - 35ms/step
Epoch 52/300

Epoch 52: val_loss improved from 0.12064 to 0.11968, saving model to ./Annotator_Model/Annotator_D1-D6_9.h5
253/253 - 10s - loss: 0.1127 - Bi-Acc: 0.3589 - val_loss: 0.1197 - val_Bi-Acc: 0.3030 - 10s/epoch - 39ms/step
Epoch 53/300

Epoch 53: val_loss did not improve from 0.11968
253/253 - 9s - loss: 0.1122 - Bi-Acc: 0.3605 - val_loss: 0.1207 - val_Bi-Acc: 0.2970 - 9s/epoch - 37ms/step
Epoch 54/300

Epoch 54: val_loss did not improve from 0.11968
253/253 - 9s - loss: 0.1116 - Bi-Acc: 0.3602 - val_loss: 0.1237 - val_Bi-Acc: 0.2788 - 9s/epoch - 35ms/step
Epoch 55/300

Epoch 55: val_loss did not improve from 0.11968
253/253 - 10s - loss: 0.1112 - Bi-Acc: 0.3609 - val_loss: 0.1200 - val_Bi-Acc: 0.2970 - 10s/epoch - 38ms/step
Epoch 56/300

Epoch 56: val_loss did not improve from 0.11968
253/253 - 10s - loss: 0.1105 - Bi-Acc: 0.3605 - val_loss: 0.1234 - val_Bi-Acc: 0.2667 - 10s/epoch - 38ms/step
Epoch 57/300

Epoch 57: val_loss did not improve from 0.11968
253/253 - 9s - loss: 0.1099 - Bi-Acc: 0.3611 - val_loss: 0.1257 - val_Bi-Acc: 0.2667 - 9s/epoch - 35ms/step
Epoch 58/300

Epoch 58: val_loss did not improve from 0.11968
253/253 - 9s - loss: 0.1096 - Bi-Acc: 0.3611 - val_loss: 0.1239 - val_Bi-Acc: 0.2727 - 9s/epoch - 37ms/step
Epoch 59/300

Epoch 59: val_loss improved from 0.11968 to 0.11788, saving model to ./Annotator_Model/Annotator_D1-D6_9.h5
253/253 - 10s - loss: 0.1087 - Bi-Acc: 0.3621 - val_loss: 0.1179 - val_Bi-Acc: 0.2909 - 10s/epoch - 40ms/step
Epoch 60/300

Epoch 60: val_loss did not improve from 0.11788
253/253 - 9s - loss: 0.1088 - Bi-Acc: 0.3618 - val_loss: 0.1205 - val_Bi-Acc: 0.2909 - 9s/epoch - 37ms/step
Epoch 61/300

Epoch 61: val_loss improved from 0.11788 to 0.11352, saving model to ./Annotator_Model/Annotator_D1-D6_9.h5
253/253 - 9s - loss: 0.1077 - Bi-Acc: 0.3640 - val_loss: 0.1135 - val_Bi-Acc: 0.3515 - 9s/epoch - 36ms/step
Epoch 62/300

Epoch 62: val_loss did not improve from 0.11352
253/253 - 10s - loss: 0.1070 - Bi-Acc: 0.3654 - val_loss: 0.1151 - val_Bi-Acc: 0.3212 - 10s/epoch - 38ms/step
Epoch 63/300

Epoch 63: val_loss did not improve from 0.11352
253/253 - 9s - loss: 0.1060 - Bi-Acc: 0.3657 - val_loss: 0.1187 - val_Bi-Acc: 0.2848 - 9s/epoch - 37ms/step
Epoch 64/300

Epoch 64: val_loss did not improve from 0.11352
253/253 - 9s - loss: 0.1062 - Bi-Acc: 0.3638 - val_loss: 0.1164 - val_Bi-Acc: 0.3091 - 9s/epoch - 36ms/step
Epoch 65/300

Epoch 65: val_loss did not improve from 0.11352
253/253 - 10s - loss: 0.1064 - Bi-Acc: 0.3637 - val_loss: 0.1155 - val_Bi-Acc: 0.3212 - 10s/epoch - 38ms/step
Epoch 66/300

Epoch 66: val_loss did not improve from 0.11352
253/253 - 9s - loss: 0.1050 - Bi-Acc: 0.3658 - val_loss: 0.1151 - val_Bi-Acc: 0.3091 - 9s/epoch - 36ms/step
Epoch 67/300

Epoch 67: val_loss did not improve from 0.11352
253/253 - 9s - loss: 0.1051 - Bi-Acc: 0.3654 - val_loss: 0.1219 - val_Bi-Acc: 0.2848 - 9s/epoch - 35ms/step
Epoch 68/300

Epoch 68: val_loss did not improve from 0.11352
253/253 - 10s - loss: 0.1052 - Bi-Acc: 0.3656 - val_loss: 0.1141 - val_Bi-Acc: 0.3152 - 10s/epoch - 38ms/step
Epoch 69/300

Epoch 69: val_loss improved from 0.11352 to 0.11207, saving model to ./Annotator_Model/Annotator_D1-D6_9.h5
253/253 - 10s - loss: 0.1040 - Bi-Acc: 0.3672 - val_loss: 0.1121 - val_Bi-Acc: 0.3152 - 10s/epoch - 40ms/step
Epoch 70/300

Epoch 70: val_loss improved from 0.11207 to 0.11017, saving model to ./Annotator_Model/Annotator_D1-D6_9.h5
253/253 - 9s - loss: 0.1036 - Bi-Acc: 0.3670 - val_loss: 0.1102 - val_Bi-Acc: 0.3394 - 9s/epoch - 37ms/step
Epoch 71/300

Epoch 71: val_loss improved from 0.11017 to 0.10988, saving model to ./Annotator_Model/Annotator_D1-D6_9.h5
253/253 - 10s - loss: 0.1032 - Bi-Acc: 0.3686 - val_loss: 0.1099 - val_Bi-Acc: 0.3333 - 10s/epoch - 38ms/step
Epoch 72/300

Epoch 72: val_loss did not improve from 0.10988
253/253 - 10s - loss: 0.1027 - Bi-Acc: 0.3685 - val_loss: 0.1103 - val_Bi-Acc: 0.3333 - 10s/epoch - 38ms/step
Epoch 73/300

Epoch 73: val_loss did not improve from 0.10988
253/253 - 9s - loss: 0.1021 - Bi-Acc: 0.3679 - val_loss: 0.1157 - val_Bi-Acc: 0.2909 - 9s/epoch - 36ms/step
Epoch 74/300

Epoch 74: val_loss did not improve from 0.10988
253/253 - 9s - loss: 0.1013 - Bi-Acc: 0.3704 - val_loss: 0.1125 - val_Bi-Acc: 0.3091 - 9s/epoch - 35ms/step
Epoch 75/300

Epoch 75: val_loss did not improve from 0.10988
253/253 - 10s - loss: 0.1009 - Bi-Acc: 0.3697 - val_loss: 0.1103 - val_Bi-Acc: 0.3394 - 10s/epoch - 39ms/step
Epoch 76/300

Epoch 76: val_loss did not improve from 0.10988
253/253 - 9s - loss: 0.1005 - Bi-Acc: 0.3699 - val_loss: 0.1124 - val_Bi-Acc: 0.3273 - 9s/epoch - 36ms/step
Epoch 77/300

Epoch 77: val_loss did not improve from 0.10988
253/253 - 9s - loss: 0.1007 - Bi-Acc: 0.3697 - val_loss: 0.1177 - val_Bi-Acc: 0.2909 - 9s/epoch - 35ms/step
Epoch 78/300

Epoch 78: val_loss did not improve from 0.10988
253/253 - 9s - loss: 0.1005 - Bi-Acc: 0.3690 - val_loss: 0.1126 - val_Bi-Acc: 0.3212 - 9s/epoch - 37ms/step
Epoch 79/300

Epoch 79: val_loss improved from 0.10988 to 0.10918, saving model to ./Annotator_Model/Annotator_D1-D6_9.h5
253/253 - 10s - loss: 0.0999 - Bi-Acc: 0.3709 - val_loss: 0.1092 - val_Bi-Acc: 0.3394 - 10s/epoch - 40ms/step
Epoch 80/300

Epoch 80: val_loss improved from 0.10918 to 0.10834, saving model to ./Annotator_Model/Annotator_D1-D6_9.h5
253/253 - 9s - loss: 0.0992 - Bi-Acc: 0.3706 - val_loss: 0.1083 - val_Bi-Acc: 0.3455 - 9s/epoch - 37ms/step
Epoch 81/300

Epoch 81: val_loss did not improve from 0.10834
253/253 - 9s - loss: 0.0992 - Bi-Acc: 0.3702 - val_loss: 0.1094 - val_Bi-Acc: 0.3515 - 9s/epoch - 37ms/step
Epoch 82/300

Epoch 82: val_loss did not improve from 0.10834
253/253 - 10s - loss: 0.0987 - Bi-Acc: 0.3714 - val_loss: 0.1113 - val_Bi-Acc: 0.3212 - 10s/epoch - 38ms/step
Epoch 83/300

Epoch 83: val_loss did not improve from 0.10834
253/253 - 9s - loss: 0.0986 - Bi-Acc: 0.3715 - val_loss: 0.1099 - val_Bi-Acc: 0.3394 - 9s/epoch - 36ms/step
Epoch 84/300

Epoch 84: val_loss did not improve from 0.10834
253/253 - 9s - loss: 0.0978 - Bi-Acc: 0.3713 - val_loss: 0.1131 - val_Bi-Acc: 0.3030 - 9s/epoch - 36ms/step
Epoch 85/300

Epoch 85: val_loss did not improve from 0.10834
253/253 - 10s - loss: 0.0975 - Bi-Acc: 0.3720 - val_loss: 0.1132 - val_Bi-Acc: 0.3091 - 10s/epoch - 38ms/step
Epoch 86/300

Epoch 86: val_loss improved from 0.10834 to 0.10833, saving model to ./Annotator_Model/Annotator_D1-D6_9.h5
253/253 - 9s - loss: 0.0967 - Bi-Acc: 0.3740 - val_loss: 0.1083 - val_Bi-Acc: 0.3333 - 9s/epoch - 37ms/step
Epoch 87/300

Epoch 87: val_loss did not improve from 0.10833
253/253 - 9s - loss: 0.0966 - Bi-Acc: 0.3733 - val_loss: 0.1122 - val_Bi-Acc: 0.3152 - 9s/epoch - 36ms/step
Epoch 88/300

Epoch 88: val_loss did not improve from 0.10833
253/253 - 10s - loss: 0.0967 - Bi-Acc: 0.3736 - val_loss: 0.1148 - val_Bi-Acc: 0.3030 - 10s/epoch - 38ms/step
Epoch 89/300

Epoch 89: val_loss did not improve from 0.10833
253/253 - 10s - loss: 0.0959 - Bi-Acc: 0.3742 - val_loss: 0.1114 - val_Bi-Acc: 0.3091 - 10s/epoch - 38ms/step
Epoch 90/300

Epoch 90: val_loss did not improve from 0.10833
253/253 - 9s - loss: 0.0963 - Bi-Acc: 0.3725 - val_loss: 0.1100 - val_Bi-Acc: 0.3273 - 9s/epoch - 35ms/step
Epoch 91/300

Epoch 91: val_loss did not improve from 0.10833
253/253 - 10s - loss: 0.0954 - Bi-Acc: 0.3746 - val_loss: 0.1100 - val_Bi-Acc: 0.3273 - 10s/epoch - 38ms/step
Epoch 92/300

Epoch 92: val_loss did not improve from 0.10833
253/253 - 9s - loss: 0.0949 - Bi-Acc: 0.3740 - val_loss: 0.1088 - val_Bi-Acc: 0.3333 - 9s/epoch - 37ms/step
Epoch 93/300

Epoch 93: val_loss improved from 0.10833 to 0.10736, saving model to ./Annotator_Model/Annotator_D1-D6_9.h5
253/253 - 9s - loss: 0.0951 - Bi-Acc: 0.3745 - val_loss: 0.1074 - val_Bi-Acc: 0.3394 - 9s/epoch - 36ms/step
Epoch 94/300

Epoch 94: val_loss did not improve from 0.10736
253/253 - 10s - loss: 0.0950 - Bi-Acc: 0.3737 - val_loss: 0.1098 - val_Bi-Acc: 0.3212 - 10s/epoch - 38ms/step
Epoch 95/300

Epoch 95: val_loss did not improve from 0.10736
253/253 - 10s - loss: 0.0936 - Bi-Acc: 0.3755 - val_loss: 0.1136 - val_Bi-Acc: 0.3152 - 10s/epoch - 38ms/step
Epoch 96/300

Epoch 96: val_loss improved from 0.10736 to 0.10578, saving model to ./Annotator_Model/Annotator_D1-D6_9.h5
253/253 - 9s - loss: 0.0941 - Bi-Acc: 0.3743 - val_loss: 0.1058 - val_Bi-Acc: 0.3394 - 9s/epoch - 36ms/step
Epoch 97/300

Epoch 97: val_loss did not improve from 0.10578
253/253 - 10s - loss: 0.0931 - Bi-Acc: 0.3765 - val_loss: 0.1063 - val_Bi-Acc: 0.3394 - 10s/epoch - 39ms/step
Epoch 98/300

Epoch 98: val_loss did not improve from 0.10578
253/253 - 10s - loss: 0.0941 - Bi-Acc: 0.3749 - val_loss: 0.1063 - val_Bi-Acc: 0.3273 - 10s/epoch - 38ms/step
Epoch 99/300

Epoch 99: val_loss improved from 0.10578 to 0.10575, saving model to ./Annotator_Model/Annotator_D1-D6_9.h5
253/253 - 10s - loss: 0.0928 - Bi-Acc: 0.3773 - val_loss: 0.1057 - val_Bi-Acc: 0.3576 - 10s/epoch - 38ms/step
Epoch 100/300

Epoch 100: val_loss improved from 0.10575 to 0.10542, saving model to ./Annotator_Model/Annotator_D1-D6_9.h5
253/253 - 10s - loss: 0.0931 - Bi-Acc: 0.3756 - val_loss: 0.1054 - val_Bi-Acc: 0.3515 - 10s/epoch - 39ms/step
Epoch 101/300

Epoch 101: val_loss did not improve from 0.10542
253/253 - 10s - loss: 0.0925 - Bi-Acc: 0.3765 - val_loss: 0.1069 - val_Bi-Acc: 0.3394 - 10s/epoch - 39ms/step
Epoch 102/300

Epoch 102: val_loss did not improve from 0.10542
253/253 - 9s - loss: 0.0922 - Bi-Acc: 0.3759 - val_loss: 0.1055 - val_Bi-Acc: 0.3515 - 9s/epoch - 36ms/step
Epoch 103/300

Epoch 103: val_loss improved from 0.10542 to 0.10455, saving model to ./Annotator_Model/Annotator_D1-D6_9.h5
253/253 - 10s - loss: 0.0919 - Bi-Acc: 0.3770 - val_loss: 0.1045 - val_Bi-Acc: 0.3576 - 10s/epoch - 38ms/step
Epoch 104/300

Epoch 104: val_loss did not improve from 0.10455
253/253 - 10s - loss: 0.0913 - Bi-Acc: 0.3772 - val_loss: 0.1077 - val_Bi-Acc: 0.3394 - 10s/epoch - 38ms/step
Epoch 105/300

Epoch 105: val_loss did not improve from 0.10455
253/253 - 9s - loss: 0.0912 - Bi-Acc: 0.3768 - val_loss: 0.1069 - val_Bi-Acc: 0.3333 - 9s/epoch - 35ms/step
Epoch 106/300

Epoch 106: val_loss did not improve from 0.10455
253/253 - 9s - loss: 0.0907 - Bi-Acc: 0.3778 - val_loss: 0.1070 - val_Bi-Acc: 0.3273 - 9s/epoch - 37ms/step
Epoch 107/300

Epoch 107: val_loss did not improve from 0.10455
253/253 - 10s - loss: 0.0904 - Bi-Acc: 0.3793 - val_loss: 0.1055 - val_Bi-Acc: 0.3455 - 10s/epoch - 38ms/step
Epoch 108/300

Epoch 108: val_loss did not improve from 0.10455
253/253 - 9s - loss: 0.0905 - Bi-Acc: 0.3771 - val_loss: 0.1083 - val_Bi-Acc: 0.3333 - 9s/epoch - 37ms/step
Epoch 109/300

Epoch 109: val_loss did not improve from 0.10455
253/253 - 9s - loss: 0.0900 - Bi-Acc: 0.3767 - val_loss: 0.1051 - val_Bi-Acc: 0.3455 - 9s/epoch - 35ms/step
Epoch 110/300

Epoch 110: val_loss did not improve from 0.10455
253/253 - 10s - loss: 0.0898 - Bi-Acc: 0.3786 - val_loss: 0.1051 - val_Bi-Acc: 0.3515 - 10s/epoch - 38ms/step
Epoch 111/300

Epoch 111: val_loss did not improve from 0.10455
253/253 - 10s - loss: 0.0890 - Bi-Acc: 0.3799 - val_loss: 0.1053 - val_Bi-Acc: 0.3394 - 10s/epoch - 38ms/step
Epoch 112/300

Epoch 112: val_loss did not improve from 0.10455
253/253 - 9s - loss: 0.0894 - Bi-Acc: 0.3773 - val_loss: 0.1051 - val_Bi-Acc: 0.3455 - 9s/epoch - 35ms/step
Epoch 113/300

Epoch 113: val_loss did not improve from 0.10455
253/253 - 10s - loss: 0.0892 - Bi-Acc: 0.3790 - val_loss: 0.1076 - val_Bi-Acc: 0.3273 - 10s/epoch - 39ms/step
Epoch 114/300

Epoch 114: val_loss did not improve from 0.10455
253/253 - 9s - loss: 0.0883 - Bi-Acc: 0.3804 - val_loss: 0.1057 - val_Bi-Acc: 0.3455 - 9s/epoch - 37ms/step
Epoch 115/300

Epoch 115: val_loss did not improve from 0.10455
253/253 - 9s - loss: 0.0884 - Bi-Acc: 0.3787 - val_loss: 0.1046 - val_Bi-Acc: 0.3576 - 9s/epoch - 36ms/step
Epoch 116/300

Epoch 116: val_loss did not improve from 0.10455
253/253 - 10s - loss: 0.0883 - Bi-Acc: 0.3782 - val_loss: 0.1046 - val_Bi-Acc: 0.3576 - 10s/epoch - 38ms/step
Epoch 117/300

Epoch 117: val_loss did not improve from 0.10455
253/253 - 10s - loss: 0.0883 - Bi-Acc: 0.3787 - val_loss: 0.1052 - val_Bi-Acc: 0.3455 - 10s/epoch - 39ms/step
Epoch 118/300

Epoch 118: val_loss did not improve from 0.10455
253/253 - 9s - loss: 0.0877 - Bi-Acc: 0.3804 - val_loss: 0.1059 - val_Bi-Acc: 0.3394 - 9s/epoch - 37ms/step
Epoch 119/300

Epoch 119: val_loss did not improve from 0.10455
253/253 - 9s - loss: 0.0874 - Bi-Acc: 0.3800 - val_loss: 0.1092 - val_Bi-Acc: 0.3273 - 9s/epoch - 36ms/step
Epoch 120/300

Epoch 120: val_loss did not improve from 0.10455
253/253 - 10s - loss: 0.0873 - Bi-Acc: 0.3799 - val_loss: 0.1074 - val_Bi-Acc: 0.3394 - 10s/epoch - 39ms/step
Epoch 121/300

Epoch 121: val_loss did not improve from 0.10455
253/253 - 9s - loss: 0.0873 - Bi-Acc: 0.3809 - val_loss: 0.1058 - val_Bi-Acc: 0.3515 - 9s/epoch - 37ms/step
Epoch 122/300

Epoch 122: val_loss did not improve from 0.10455
253/253 - 9s - loss: 0.0864 - Bi-Acc: 0.3805 - val_loss: 0.1091 - val_Bi-Acc: 0.3394 - 9s/epoch - 36ms/step
Epoch 123/300

Epoch 123: val_loss did not improve from 0.10455
253/253 - 10s - loss: 0.0871 - Bi-Acc: 0.3803 - val_loss: 0.1050 - val_Bi-Acc: 0.3576 - 10s/epoch - 39ms/step
Epoch 124/300

Epoch 124: val_loss did not improve from 0.10455
253/253 - 9s - loss: 0.0863 - Bi-Acc: 0.3806 - val_loss: 0.1051 - val_Bi-Acc: 0.3515 - 9s/epoch - 36ms/step
Epoch 125/300

Epoch 125: val_loss did not improve from 0.10455
253/253 - 9s - loss: 0.0860 - Bi-Acc: 0.3806 - val_loss: 0.1060 - val_Bi-Acc: 0.3394 - 9s/epoch - 36ms/step
Epoch 126/300

Epoch 126: val_loss improved from 0.10455 to 0.10337, saving model to ./Annotator_Model/Annotator_D1-D6_9.h5
253/253 - 10s - loss: 0.0858 - Bi-Acc: 0.3815 - val_loss: 0.1034 - val_Bi-Acc: 0.3515 - 10s/epoch - 40ms/step
Epoch 127/300

Epoch 127: val_loss did not improve from 0.10337
253/253 - 10s - loss: 0.0856 - Bi-Acc: 0.3821 - val_loss: 0.1040 - val_Bi-Acc: 0.3576 - 10s/epoch - 38ms/step
Epoch 128/300

Epoch 128: val_loss did not improve from 0.10337
253/253 - 9s - loss: 0.0854 - Bi-Acc: 0.3828 - val_loss: 0.1054 - val_Bi-Acc: 0.3515 - 9s/epoch - 36ms/step
Epoch 129/300

Epoch 129: val_loss did not improve from 0.10337
253/253 - 9s - loss: 0.0852 - Bi-Acc: 0.3824 - val_loss: 0.1055 - val_Bi-Acc: 0.3515 - 9s/epoch - 37ms/step
Epoch 130/300

Epoch 130: val_loss did not improve from 0.10337
253/253 - 10s - loss: 0.0854 - Bi-Acc: 0.3815 - val_loss: 0.1050 - val_Bi-Acc: 0.3576 - 10s/epoch - 38ms/step
Epoch 131/300

Epoch 131: val_loss did not improve from 0.10337
253/253 - 9s - loss: 0.0851 - Bi-Acc: 0.3819 - val_loss: 0.1051 - val_Bi-Acc: 0.3576 - 9s/epoch - 36ms/step
Epoch 132/300

Epoch 132: val_loss did not improve from 0.10337
253/253 - 9s - loss: 0.0845 - Bi-Acc: 0.3825 - val_loss: 0.1068 - val_Bi-Acc: 0.3455 - 9s/epoch - 37ms/step
Epoch 133/300

Epoch 133: val_loss did not improve from 0.10337
253/253 - 10s - loss: 0.0845 - Bi-Acc: 0.3817 - val_loss: 0.1048 - val_Bi-Acc: 0.3515 - 10s/epoch - 38ms/step
Epoch 134/300

Epoch 134: val_loss did not improve from 0.10337
253/253 - 9s - loss: 0.0835 - Bi-Acc: 0.3834 - val_loss: 0.1068 - val_Bi-Acc: 0.3455 - 9s/epoch - 36ms/step
Epoch 135/300

Epoch 135: val_loss did not improve from 0.10337
253/253 - 9s - loss: 0.0839 - Bi-Acc: 0.3832 - val_loss: 0.1042 - val_Bi-Acc: 0.3576 - 9s/epoch - 37ms/step
Epoch 136/300

Epoch 136: val_loss did not improve from 0.10337
253/253 - 10s - loss: 0.0838 - Bi-Acc: 0.3832 - val_loss: 0.1058 - val_Bi-Acc: 0.3455 - 10s/epoch - 38ms/step
Epoch 137/300

Epoch 137: val_loss did not improve from 0.10337
253/253 - 9s - loss: 0.0834 - Bi-Acc: 0.3833 - val_loss: 0.1051 - val_Bi-Acc: 0.3515 - 9s/epoch - 37ms/step
Epoch 138/300

Epoch 138: val_loss did not improve from 0.10337
253/253 - 9s - loss: 0.0837 - Bi-Acc: 0.3826 - val_loss: 0.1049 - val_Bi-Acc: 0.3515 - 9s/epoch - 35ms/step
Epoch 139/300

Epoch 139: val_loss improved from 0.10337 to 0.10329, saving model to ./Annotator_Model/Annotator_D1-D6_9.h5
253/253 - 10s - loss: 0.0833 - Bi-Acc: 0.3829 - val_loss: 0.1033 - val_Bi-Acc: 0.3576 - 10s/epoch - 39ms/step
Epoch 140/300

Epoch 140: val_loss did not improve from 0.10329
253/253 - 9s - loss: 0.0835 - Bi-Acc: 0.3831 - val_loss: 0.1080 - val_Bi-Acc: 0.3394 - 9s/epoch - 37ms/step
Epoch 141/300

Epoch 141: val_loss did not improve from 0.10329
253/253 - 9s - loss: 0.0824 - Bi-Acc: 0.3850 - val_loss: 0.1051 - val_Bi-Acc: 0.3576 - 9s/epoch - 35ms/step
Epoch 142/300

Epoch 142: val_loss did not improve from 0.10329
253/253 - 10s - loss: 0.0826 - Bi-Acc: 0.3839 - val_loss: 0.1051 - val_Bi-Acc: 0.3576 - 10s/epoch - 38ms/step
Epoch 143/300

Epoch 143: val_loss did not improve from 0.10329
253/253 - 9s - loss: 0.0822 - Bi-Acc: 0.3839 - val_loss: 0.1046 - val_Bi-Acc: 0.3515 - 9s/epoch - 36ms/step
Epoch 144/300

Epoch 144: val_loss did not improve from 0.10329
253/253 - 9s - loss: 0.0822 - Bi-Acc: 0.3839 - val_loss: 0.1061 - val_Bi-Acc: 0.3455 - 9s/epoch - 35ms/step
Epoch 145/300

Epoch 145: val_loss did not improve from 0.10329
253/253 - 10s - loss: 0.0814 - Bi-Acc: 0.3846 - val_loss: 0.1068 - val_Bi-Acc: 0.3394 - 10s/epoch - 38ms/step
Epoch 146/300

Epoch 146: val_loss did not improve from 0.10329
253/253 - 10s - loss: 0.0822 - Bi-Acc: 0.3844 - val_loss: 0.1053 - val_Bi-Acc: 0.3455 - 10s/epoch - 38ms/step
Epoch 147/300

Epoch 147: val_loss did not improve from 0.10329
253/253 - 9s - loss: 0.0817 - Bi-Acc: 0.3847 - val_loss: 0.1051 - val_Bi-Acc: 0.3455 - 9s/epoch - 36ms/step
Epoch 148/300

Epoch 148: val_loss did not improve from 0.10329
253/253 - 9s - loss: 0.0815 - Bi-Acc: 0.3850 - val_loss: 0.1044 - val_Bi-Acc: 0.3455 - 9s/epoch - 37ms/step
Epoch 149/300

Epoch 149: val_loss did not improve from 0.10329
253/253 - 10s - loss: 0.0810 - Bi-Acc: 0.3849 - val_loss: 0.1061 - val_Bi-Acc: 0.3455 - 10s/epoch - 39ms/step
Epoch 150/300

Epoch 150: val_loss did not improve from 0.10329
253/253 - 9s - loss: 0.0811 - Bi-Acc: 0.3852 - val_loss: 0.1088 - val_Bi-Acc: 0.3394 - 9s/epoch - 36ms/step
Epoch 151/300

Epoch 151: val_loss did not improve from 0.10329
253/253 - 9s - loss: 0.0808 - Bi-Acc: 0.3854 - val_loss: 0.1058 - val_Bi-Acc: 0.3455 - 9s/epoch - 36ms/step
Epoch 152/300

Epoch 152: val_loss did not improve from 0.10329
253/253 - 10s - loss: 0.0807 - Bi-Acc: 0.3850 - val_loss: 0.1061 - val_Bi-Acc: 0.3455 - 10s/epoch - 38ms/step
Epoch 153/300

Epoch 153: val_loss did not improve from 0.10329
253/253 - 9s - loss: 0.0803 - Bi-Acc: 0.3856 - val_loss: 0.1064 - val_Bi-Acc: 0.3455 - 9s/epoch - 37ms/step
Epoch 154/300

Epoch 154: val_loss did not improve from 0.10329
253/253 - 9s - loss: 0.0805 - Bi-Acc: 0.3859 - val_loss: 0.1054 - val_Bi-Acc: 0.3455 - 9s/epoch - 36ms/step
Epoch 155/300

Epoch 155: val_loss did not improve from 0.10329
253/253 - 10s - loss: 0.0800 - Bi-Acc: 0.3857 - val_loss: 0.1077 - val_Bi-Acc: 0.3455 - 10s/epoch - 38ms/step
Epoch 156/300

Epoch 156: val_loss did not improve from 0.10329
253/253 - 10s - loss: 0.0802 - Bi-Acc: 0.3860 - val_loss: 0.1058 - val_Bi-Acc: 0.3515 - 10s/epoch - 38ms/step
Epoch 157/300

Epoch 157: val_loss did not improve from 0.10329
253/253 - 9s - loss: 0.0798 - Bi-Acc: 0.3867 - val_loss: 0.1054 - val_Bi-Acc: 0.3576 - 9s/epoch - 35ms/step
Epoch 158/300

Epoch 158: val_loss did not improve from 0.10329
253/253 - 9s - loss: 0.0797 - Bi-Acc: 0.3856 - val_loss: 0.1061 - val_Bi-Acc: 0.3576 - 9s/epoch - 37ms/step
Epoch 159/300

Epoch 159: val_loss did not improve from 0.10329
253/253 - 10s - loss: 0.0792 - Bi-Acc: 0.3859 - val_loss: 0.1072 - val_Bi-Acc: 0.3455 - 10s/epoch - 38ms/step
Epoch 160/300

Epoch 160: val_loss did not improve from 0.10329
253/253 - 9s - loss: 0.0793 - Bi-Acc: 0.3859 - val_loss: 0.1075 - val_Bi-Acc: 0.3394 - 9s/epoch - 35ms/step
Epoch 161/300

Epoch 161: val_loss did not improve from 0.10329
253/253 - 9s - loss: 0.0788 - Bi-Acc: 0.3869 - val_loss: 0.1041 - val_Bi-Acc: 0.3576 - 9s/epoch - 37ms/step
Epoch 162/300

Epoch 162: val_loss did not improve from 0.10329
253/253 - 10s - loss: 0.0786 - Bi-Acc: 0.3864 - val_loss: 0.1048 - val_Bi-Acc: 0.3576 - 10s/epoch - 38ms/step
Epoch 163/300

Epoch 163: val_loss did not improve from 0.10329
253/253 - 9s - loss: 0.0789 - Bi-Acc: 0.3862 - val_loss: 0.1084 - val_Bi-Acc: 0.3394 - 9s/epoch - 36ms/step
Epoch 164/300

Epoch 164: val_loss did not improve from 0.10329
253/253 - 9s - loss: 0.0785 - Bi-Acc: 0.3871 - val_loss: 0.1065 - val_Bi-Acc: 0.3515 - 9s/epoch - 36ms/step
Epoch 165/300

Epoch 165: val_loss did not improve from 0.10329
253/253 - 10s - loss: 0.0785 - Bi-Acc: 0.3861 - val_loss: 0.1054 - val_Bi-Acc: 0.3455 - 10s/epoch - 38ms/step
Epoch 166/300

Epoch 166: val_loss did not improve from 0.10329
253/253 - 9s - loss: 0.0782 - Bi-Acc: 0.3865 - val_loss: 0.1056 - val_Bi-Acc: 0.3515 - 9s/epoch - 37ms/step
Epoch 167/300

Epoch 167: val_loss did not improve from 0.10329
253/253 - 9s - loss: 0.0788 - Bi-Acc: 0.3856 - val_loss: 0.1040 - val_Bi-Acc: 0.3576 - 9s/epoch - 36ms/step
Epoch 168/300

Epoch 168: val_loss did not improve from 0.10329
253/253 - 10s - loss: 0.0778 - Bi-Acc: 0.3876 - val_loss: 0.1058 - val_Bi-Acc: 0.3515 - 10s/epoch - 38ms/step
Epoch 169/300

Epoch 169: val_loss did not improve from 0.10329
253/253 - 10s - loss: 0.0778 - Bi-Acc: 0.3872 - val_loss: 0.1061 - val_Bi-Acc: 0.3515 - 10s/epoch - 38ms/step
Epoch 170/300

Epoch 170: val_loss did not improve from 0.10329
253/253 - 9s - loss: 0.0778 - Bi-Acc: 0.3870 - val_loss: 0.1062 - val_Bi-Acc: 0.3576 - 9s/epoch - 35ms/step
Epoch 171/300

Epoch 171: val_loss did not improve from 0.10329
253/253 - 10s - loss: 0.0772 - Bi-Acc: 0.3872 - val_loss: 0.1041 - val_Bi-Acc: 0.3515 - 10s/epoch - 38ms/step
Epoch 172/300

Epoch 172: val_loss did not improve from 0.10329
253/253 - 9s - loss: 0.0777 - Bi-Acc: 0.3870 - val_loss: 0.1055 - val_Bi-Acc: 0.3576 - 9s/epoch - 37ms/step
Epoch 173/300

Epoch 173: val_loss did not improve from 0.10329
253/253 - 9s - loss: 0.0772 - Bi-Acc: 0.3866 - val_loss: 0.1060 - val_Bi-Acc: 0.3576 - 9s/epoch - 35ms/step
Epoch 174/300

Epoch 174: val_loss did not improve from 0.10329
253/253 - 9s - loss: 0.0771 - Bi-Acc: 0.3876 - val_loss: 0.1065 - val_Bi-Acc: 0.3515 - 9s/epoch - 37ms/step
Epoch 175/300

Epoch 175: val_loss did not improve from 0.10329
253/253 - 10s - loss: 0.0773 - Bi-Acc: 0.3873 - val_loss: 0.1050 - val_Bi-Acc: 0.3576 - 10s/epoch - 38ms/step
Epoch 176/300

Epoch 176: val_loss did not improve from 0.10329
253/253 - 9s - loss: 0.0773 - Bi-Acc: 0.3877 - val_loss: 0.1062 - val_Bi-Acc: 0.3394 - 9s/epoch - 37ms/step
Epoch 177/300

Epoch 177: val_loss did not improve from 0.10329
253/253 - 9s - loss: 0.0769 - Bi-Acc: 0.3881 - val_loss: 0.1059 - val_Bi-Acc: 0.3515 - 9s/epoch - 35ms/step
Epoch 178/300

Epoch 178: val_loss did not improve from 0.10329
253/253 - 10s - loss: 0.0763 - Bi-Acc: 0.3877 - val_loss: 0.1080 - val_Bi-Acc: 0.3394 - 10s/epoch - 39ms/step
Epoch 179/300

Epoch 179: val_loss did not improve from 0.10329
253/253 - 10s - loss: 0.0762 - Bi-Acc: 0.3873 - val_loss: 0.1052 - val_Bi-Acc: 0.3515 - 10s/epoch - 38ms/step
Epoch 180/300

Epoch 180: val_loss did not improve from 0.10329
253/253 - 9s - loss: 0.0765 - Bi-Acc: 0.3873 - val_loss: 0.1036 - val_Bi-Acc: 0.3576 - 9s/epoch - 36ms/step
Epoch 181/300

Epoch 181: val_loss improved from 0.10329 to 0.10302, saving model to ./Annotator_Model/Annotator_D1-D6_9.h5
253/253 - 10s - loss: 0.0763 - Bi-Acc: 0.3888 - val_loss: 0.1030 - val_Bi-Acc: 0.3576 - 10s/epoch - 39ms/step
Epoch 182/300

Epoch 182: val_loss did not improve from 0.10302
253/253 - 9s - loss: 0.0761 - Bi-Acc: 0.3877 - val_loss: 0.1046 - val_Bi-Acc: 0.3576 - 9s/epoch - 36ms/step
Epoch 183/300

Epoch 183: val_loss did not improve from 0.10302
253/253 - 9s - loss: 0.0753 - Bi-Acc: 0.3895 - val_loss: 0.1084 - val_Bi-Acc: 0.3394 - 9s/epoch - 36ms/step
Epoch 184/300

Epoch 184: val_loss did not improve from 0.10302
253/253 - 10s - loss: 0.0758 - Bi-Acc: 0.3880 - val_loss: 0.1053 - val_Bi-Acc: 0.3515 - 10s/epoch - 38ms/step
Epoch 185/300

Epoch 185: val_loss did not improve from 0.10302
253/253 - 9s - loss: 0.0756 - Bi-Acc: 0.3876 - val_loss: 0.1076 - val_Bi-Acc: 0.3333 - 9s/epoch - 37ms/step
Epoch 186/300

Epoch 186: val_loss did not improve from 0.10302
253/253 - 9s - loss: 0.0757 - Bi-Acc: 0.3881 - val_loss: 0.1044 - val_Bi-Acc: 0.3576 - 9s/epoch - 35ms/step
Epoch 187/300

Epoch 187: val_loss did not improve from 0.10302
253/253 - 9s - loss: 0.0749 - Bi-Acc: 0.3893 - val_loss: 0.1054 - val_Bi-Acc: 0.3515 - 9s/epoch - 37ms/step
Epoch 188/300

Epoch 188: val_loss did not improve from 0.10302
253/253 - 10s - loss: 0.0753 - Bi-Acc: 0.3886 - val_loss: 0.1043 - val_Bi-Acc: 0.3576 - 10s/epoch - 39ms/step
Epoch 189/300

Epoch 189: val_loss did not improve from 0.10302
253/253 - 9s - loss: 0.0752 - Bi-Acc: 0.3883 - val_loss: 0.1036 - val_Bi-Acc: 0.3576 - 9s/epoch - 36ms/step
Epoch 190/300

Epoch 190: val_loss did not improve from 0.10302
253/253 - 10s - loss: 0.0750 - Bi-Acc: 0.3881 - val_loss: 0.1057 - val_Bi-Acc: 0.3394 - 10s/epoch - 38ms/step
Epoch 191/300

Epoch 191: val_loss did not improve from 0.10302
253/253 - 10s - loss: 0.0747 - Bi-Acc: 0.3882 - val_loss: 0.1060 - val_Bi-Acc: 0.3515 - 10s/epoch - 38ms/step
Epoch 192/300

Epoch 192: val_loss did not improve from 0.10302
253/253 - 9s - loss: 0.0745 - Bi-Acc: 0.3890 - val_loss: 0.1042 - val_Bi-Acc: 0.3576 - 9s/epoch - 35ms/step
Epoch 193/300

Epoch 193: val_loss did not improve from 0.10302
253/253 - 9s - loss: 0.0747 - Bi-Acc: 0.3886 - val_loss: 0.1036 - val_Bi-Acc: 0.3576 - 9s/epoch - 37ms/step
Epoch 194/300

Epoch 194: val_loss did not improve from 0.10302
253/253 - 10s - loss: 0.0745 - Bi-Acc: 0.3893 - val_loss: 0.1070 - val_Bi-Acc: 0.3394 - 10s/epoch - 38ms/step
Epoch 195/300

Epoch 195: val_loss did not improve from 0.10302
253/253 - 9s - loss: 0.0741 - Bi-Acc: 0.3894 - val_loss: 0.1034 - val_Bi-Acc: 0.3636 - 9s/epoch - 36ms/step
Epoch 196/300

Epoch 196: val_loss did not improve from 0.10302
253/253 - 9s - loss: 0.0743 - Bi-Acc: 0.3889 - val_loss: 0.1047 - val_Bi-Acc: 0.3576 - 9s/epoch - 36ms/step
Epoch 197/300

Epoch 197: val_loss did not improve from 0.10302
253/253 - 10s - loss: 0.0742 - Bi-Acc: 0.3890 - val_loss: 0.1067 - val_Bi-Acc: 0.3394 - 10s/epoch - 38ms/step
Epoch 198/300

Epoch 198: val_loss did not improve from 0.10302
253/253 - 9s - loss: 0.0740 - Bi-Acc: 0.3893 - val_loss: 0.1034 - val_Bi-Acc: 0.3576 - 9s/epoch - 37ms/step
Epoch 199/300

Epoch 199: val_loss did not improve from 0.10302
253/253 - 9s - loss: 0.0741 - Bi-Acc: 0.3893 - val_loss: 0.1067 - val_Bi-Acc: 0.3455 - 9s/epoch - 35ms/step
Epoch 200/300

Epoch 200: val_loss did not improve from 0.10302
253/253 - 9s - loss: 0.0736 - Bi-Acc: 0.3893 - val_loss: 0.1059 - val_Bi-Acc: 0.3394 - 9s/epoch - 37ms/step
Epoch 201/300

Epoch 201: val_loss did not improve from 0.10302
253/253 - 9s - loss: 0.0734 - Bi-Acc: 0.3898 - val_loss: 0.1035 - val_Bi-Acc: 0.3636 - 9s/epoch - 37ms/step
Epoch 202/300

Epoch 202: val_loss did not improve from 0.10302
253/253 - 9s - loss: 0.0735 - Bi-Acc: 0.3896 - val_loss: 0.1072 - val_Bi-Acc: 0.3333 - 9s/epoch - 35ms/step
Epoch 203/300

Epoch 203: val_loss did not improve from 0.10302
253/253 - 9s - loss: 0.0731 - Bi-Acc: 0.3902 - val_loss: 0.1048 - val_Bi-Acc: 0.3455 - 9s/epoch - 37ms/step
Epoch 204/300

Epoch 204: val_loss did not improve from 0.10302
253/253 - 10s - loss: 0.0732 - Bi-Acc: 0.3904 - val_loss: 0.1049 - val_Bi-Acc: 0.3515 - 10s/epoch - 39ms/step
Epoch 205/300

Epoch 205: val_loss did not improve from 0.10302
253/253 - 9s - loss: 0.0727 - Bi-Acc: 0.3903 - val_loss: 0.1050 - val_Bi-Acc: 0.3455 - 9s/epoch - 36ms/step
Epoch 206/300

Epoch 206: val_loss did not improve from 0.10302
253/253 - 9s - loss: 0.0725 - Bi-Acc: 0.3903 - val_loss: 0.1041 - val_Bi-Acc: 0.3515 - 9s/epoch - 36ms/step
Epoch 207/300

Epoch 207: val_loss improved from 0.10302 to 0.10266, saving model to ./Annotator_Model/Annotator_D1-D6_9.h5
253/253 - 10s - loss: 0.0727 - Bi-Acc: 0.3903 - val_loss: 0.1027 - val_Bi-Acc: 0.3636 - 10s/epoch - 40ms/step
Epoch 208/300

Epoch 208: val_loss did not improve from 0.10266
253/253 - 9s - loss: 0.0722 - Bi-Acc: 0.3901 - val_loss: 0.1040 - val_Bi-Acc: 0.3515 - 9s/epoch - 36ms/step
Epoch 209/300

Epoch 209: val_loss improved from 0.10266 to 0.10185, saving model to ./Annotator_Model/Annotator_D1-D6_9.h5
253/253 - 9s - loss: 0.0726 - Bi-Acc: 0.3895 - val_loss: 0.1019 - val_Bi-Acc: 0.3636 - 9s/epoch - 37ms/step
Epoch 210/300

Epoch 210: val_loss did not improve from 0.10185
253/253 - 10s - loss: 0.0724 - Bi-Acc: 0.3904 - val_loss: 0.1051 - val_Bi-Acc: 0.3515 - 10s/epoch - 38ms/step
Epoch 211/300

Epoch 211: val_loss did not improve from 0.10185
253/253 - 9s - loss: 0.0725 - Bi-Acc: 0.3897 - val_loss: 0.1036 - val_Bi-Acc: 0.3576 - 9s/epoch - 37ms/step
Epoch 212/300

Epoch 212: val_loss did not improve from 0.10185
253/253 - 9s - loss: 0.0723 - Bi-Acc: 0.3907 - val_loss: 0.1063 - val_Bi-Acc: 0.3333 - 9s/epoch - 35ms/step
Epoch 213/300

Epoch 213: val_loss did not improve from 0.10185
253/253 - 9s - loss: 0.0725 - Bi-Acc: 0.3899 - val_loss: 0.1050 - val_Bi-Acc: 0.3636 - 9s/epoch - 37ms/step
Epoch 214/300

Epoch 214: val_loss did not improve from 0.10185
253/253 - 9s - loss: 0.0719 - Bi-Acc: 0.3904 - val_loss: 0.1059 - val_Bi-Acc: 0.3515 - 9s/epoch - 37ms/step
Epoch 215/300

Epoch 215: val_loss did not improve from 0.10185
253/253 - 9s - loss: 0.0719 - Bi-Acc: 0.3905 - val_loss: 0.1057 - val_Bi-Acc: 0.3576 - 9s/epoch - 36ms/step
Epoch 216/300

Epoch 216: val_loss did not improve from 0.10185
253/253 - 10s - loss: 0.0722 - Bi-Acc: 0.3906 - val_loss: 0.1061 - val_Bi-Acc: 0.3636 - 10s/epoch - 38ms/step
Epoch 217/300

Epoch 217: val_loss did not improve from 0.10185
253/253 - 10s - loss: 0.0718 - Bi-Acc: 0.3907 - val_loss: 0.1066 - val_Bi-Acc: 0.3515 - 10s/epoch - 38ms/step
Epoch 218/300

Epoch 218: val_loss did not improve from 0.10185
253/253 - 9s - loss: 0.0714 - Bi-Acc: 0.3911 - val_loss: 0.1031 - val_Bi-Acc: 0.3636 - 9s/epoch - 35ms/step
Epoch 219/300

Epoch 219: val_loss did not improve from 0.10185
253/253 - 9s - loss: 0.0721 - Bi-Acc: 0.3904 - val_loss: 0.1065 - val_Bi-Acc: 0.3515 - 9s/epoch - 37ms/step
Epoch 220/300

Epoch 220: val_loss did not improve from 0.10185
253/253 - 10s - loss: 0.0714 - Bi-Acc: 0.3909 - val_loss: 0.1061 - val_Bi-Acc: 0.3515 - 10s/epoch - 38ms/step
Epoch 221/300

Epoch 221: val_loss did not improve from 0.10185
253/253 - 9s - loss: 0.0708 - Bi-Acc: 0.3916 - val_loss: 0.1064 - val_Bi-Acc: 0.3333 - 9s/epoch - 35ms/step
Epoch 222/300

Epoch 222: val_loss did not improve from 0.10185
253/253 - 9s - loss: 0.0708 - Bi-Acc: 0.3918 - val_loss: 0.1039 - val_Bi-Acc: 0.3576 - 9s/epoch - 37ms/step
Epoch 223/300

Epoch 223: val_loss did not improve from 0.10185
253/253 - 10s - loss: 0.0710 - Bi-Acc: 0.3914 - val_loss: 0.1085 - val_Bi-Acc: 0.3333 - 10s/epoch - 38ms/step
Epoch 224/300

Epoch 224: val_loss did not improve from 0.10185
253/253 - 9s - loss: 0.0704 - Bi-Acc: 0.3917 - val_loss: 0.1052 - val_Bi-Acc: 0.3515 - 9s/epoch - 36ms/step
Epoch 225/300

Epoch 225: val_loss did not improve from 0.10185
253/253 - 9s - loss: 0.0706 - Bi-Acc: 0.3911 - val_loss: 0.1062 - val_Bi-Acc: 0.3455 - 9s/epoch - 35ms/step
Epoch 226/300

Epoch 226: val_loss did not improve from 0.10185
253/253 - 10s - loss: 0.0706 - Bi-Acc: 0.3910 - val_loss: 0.1071 - val_Bi-Acc: 0.3515 - 10s/epoch - 38ms/step
Epoch 227/300

Epoch 227: val_loss did not improve from 0.10185
253/253 - 10s - loss: 0.0709 - Bi-Acc: 0.3910 - val_loss: 0.1062 - val_Bi-Acc: 0.3515 - 10s/epoch - 38ms/step
Epoch 228/300

Epoch 228: val_loss did not improve from 0.10185
253/253 - 9s - loss: 0.0707 - Bi-Acc: 0.3914 - val_loss: 0.1076 - val_Bi-Acc: 0.3576 - 9s/epoch - 35ms/step
Epoch 229/300

Epoch 229: val_loss did not improve from 0.10185
253/253 - 10s - loss: 0.0705 - Bi-Acc: 0.3912 - val_loss: 0.1059 - val_Bi-Acc: 0.3576 - 10s/epoch - 38ms/step
Epoch 230/300

Epoch 230: val_loss did not improve from 0.10185
253/253 - 10s - loss: 0.0703 - Bi-Acc: 0.3916 - val_loss: 0.1061 - val_Bi-Acc: 0.3576 - 10s/epoch - 39ms/step
Epoch 231/300

Epoch 231: val_loss did not improve from 0.10185
253/253 - 9s - loss: 0.0704 - Bi-Acc: 0.3912 - val_loss: 0.1090 - val_Bi-Acc: 0.3333 - 9s/epoch - 35ms/step
Epoch 232/300

Epoch 232: val_loss did not improve from 0.10185
253/253 - 9s - loss: 0.0699 - Bi-Acc: 0.3915 - val_loss: 0.1078 - val_Bi-Acc: 0.3333 - 9s/epoch - 37ms/step
Epoch 233/300

Epoch 233: val_loss did not improve from 0.10185
253/253 - 10s - loss: 0.0701 - Bi-Acc: 0.3912 - val_loss: 0.1070 - val_Bi-Acc: 0.3455 - 10s/epoch - 38ms/step
Epoch 234/300

Epoch 234: val_loss did not improve from 0.10185
253/253 - 9s - loss: 0.0699 - Bi-Acc: 0.3918 - val_loss: 0.1073 - val_Bi-Acc: 0.3515 - 9s/epoch - 36ms/step
Epoch 235/300

Epoch 235: val_loss did not improve from 0.10185
253/253 - 9s - loss: 0.0699 - Bi-Acc: 0.3912 - val_loss: 0.1081 - val_Bi-Acc: 0.3515 - 9s/epoch - 36ms/step
Epoch 236/300

Epoch 236: val_loss did not improve from 0.10185
253/253 - 10s - loss: 0.0696 - Bi-Acc: 0.3919 - val_loss: 0.1071 - val_Bi-Acc: 0.3515 - 10s/epoch - 39ms/step
Epoch 237/300

Epoch 237: val_loss did not improve from 0.10185
253/253 - 9s - loss: 0.0696 - Bi-Acc: 0.3914 - val_loss: 0.1062 - val_Bi-Acc: 0.3455 - 9s/epoch - 37ms/step
Epoch 238/300

Epoch 238: val_loss did not improve from 0.10185
253/253 - 9s - loss: 0.0694 - Bi-Acc: 0.3923 - val_loss: 0.1090 - val_Bi-Acc: 0.3455 - 9s/epoch - 37ms/step
Epoch 239/300

Epoch 239: val_loss did not improve from 0.10185
253/253 - 10s - loss: 0.0696 - Bi-Acc: 0.3915 - val_loss: 0.1084 - val_Bi-Acc: 0.3455 - 10s/epoch - 38ms/step
Epoch 240/300

Epoch 240: val_loss did not improve from 0.10185
253/253 - 9s - loss: 0.0695 - Bi-Acc: 0.3922 - val_loss: 0.1050 - val_Bi-Acc: 0.3576 - 9s/epoch - 36ms/step
Epoch 241/300

Epoch 241: val_loss did not improve from 0.10185
253/253 - 9s - loss: 0.0691 - Bi-Acc: 0.3924 - val_loss: 0.1064 - val_Bi-Acc: 0.3576 - 9s/epoch - 36ms/step
Epoch 242/300

Epoch 242: val_loss did not improve from 0.10185
253/253 - 10s - loss: 0.0689 - Bi-Acc: 0.3916 - val_loss: 0.1073 - val_Bi-Acc: 0.3576 - 10s/epoch - 38ms/step
Epoch 243/300

Epoch 243: val_loss did not improve from 0.10185
253/253 - 10s - loss: 0.0691 - Bi-Acc: 0.3914 - val_loss: 0.1060 - val_Bi-Acc: 0.3515 - 10s/epoch - 38ms/step
Epoch 244/300

Epoch 244: val_loss did not improve from 0.10185
253/253 - 9s - loss: 0.0692 - Bi-Acc: 0.3908 - val_loss: 0.1071 - val_Bi-Acc: 0.3455 - 9s/epoch - 35ms/step
Epoch 245/300

Epoch 245: val_loss did not improve from 0.10185
253/253 - 10s - loss: 0.0691 - Bi-Acc: 0.3916 - val_loss: 0.1055 - val_Bi-Acc: 0.3515 - 10s/epoch - 38ms/step
Epoch 246/300

Epoch 246: val_loss did not improve from 0.10185
253/253 - 9s - loss: 0.0688 - Bi-Acc: 0.3920 - val_loss: 0.1068 - val_Bi-Acc: 0.3455 - 9s/epoch - 37ms/step
Epoch 247/300

Epoch 247: val_loss did not improve from 0.10185
253/253 - 9s - loss: 0.0683 - Bi-Acc: 0.3927 - val_loss: 0.1061 - val_Bi-Acc: 0.3576 - 9s/epoch - 35ms/step
Epoch 248/300

Epoch 248: val_loss did not improve from 0.10185
253/253 - 10s - loss: 0.0688 - Bi-Acc: 0.3918 - val_loss: 0.1069 - val_Bi-Acc: 0.3455 - 10s/epoch - 38ms/step
Epoch 249/300

Epoch 249: val_loss did not improve from 0.10185
253/253 - 10s - loss: 0.0682 - Bi-Acc: 0.3921 - val_loss: 0.1089 - val_Bi-Acc: 0.3394 - 10s/epoch - 38ms/step
Epoch 250/300

Epoch 250: val_loss did not improve from 0.10185
253/253 - 9s - loss: 0.0686 - Bi-Acc: 0.3921 - val_loss: 0.1053 - val_Bi-Acc: 0.3576 - 9s/epoch - 37ms/step
Epoch 251/300

Epoch 251: val_loss did not improve from 0.10185
253/253 - 9s - loss: 0.0684 - Bi-Acc: 0.3918 - val_loss: 0.1059 - val_Bi-Acc: 0.3515 - 9s/epoch - 37ms/step
Epoch 252/300

Epoch 252: val_loss did not improve from 0.10185
253/253 - 10s - loss: 0.0683 - Bi-Acc: 0.3923 - val_loss: 0.1059 - val_Bi-Acc: 0.3576 - 10s/epoch - 39ms/step
Epoch 253/300

Epoch 253: val_loss did not improve from 0.10185
253/253 - 9s - loss: 0.0685 - Bi-Acc: 0.3922 - val_loss: 0.1058 - val_Bi-Acc: 0.3455 - 9s/epoch - 36ms/step
Epoch 254/300

Epoch 254: val_loss did not improve from 0.10185
253/253 - 9s - loss: 0.0687 - Bi-Acc: 0.3918 - val_loss: 0.1072 - val_Bi-Acc: 0.3515 - 9s/epoch - 36ms/step
Epoch 255/300

Epoch 255: val_loss did not improve from 0.10185
253/253 - 10s - loss: 0.0677 - Bi-Acc: 0.3923 - val_loss: 0.1078 - val_Bi-Acc: 0.3455 - 10s/epoch - 38ms/step
Epoch 256/300

Epoch 256: val_loss did not improve from 0.10185
253/253 - 9s - loss: 0.0681 - Bi-Acc: 0.3927 - val_loss: 0.1085 - val_Bi-Acc: 0.3455 - 9s/epoch - 37ms/step
Epoch 257/300

Epoch 257: val_loss did not improve from 0.10185
253/253 - 9s - loss: 0.0678 - Bi-Acc: 0.3924 - val_loss: 0.1062 - val_Bi-Acc: 0.3576 - 9s/epoch - 36ms/step
Epoch 258/300

Epoch 258: val_loss did not improve from 0.10185
253/253 - 10s - loss: 0.0678 - Bi-Acc: 0.3925 - val_loss: 0.1078 - val_Bi-Acc: 0.3333 - 10s/epoch - 38ms/step
Epoch 259/300

Epoch 259: val_loss did not improve from 0.10185
253/253 - 9s - loss: 0.0675 - Bi-Acc: 0.3929 - val_loss: 0.1076 - val_Bi-Acc: 0.3394 - 9s/epoch - 36ms/step
Epoch 260/300

Epoch 260: val_loss did not improve from 0.10185
253/253 - 9s - loss: 0.0676 - Bi-Acc: 0.3920 - val_loss: 0.1083 - val_Bi-Acc: 0.3455 - 9s/epoch - 35ms/step
Epoch 261/300

Epoch 261: val_loss did not improve from 0.10185
253/253 - 10s - loss: 0.0679 - Bi-Acc: 0.3924 - val_loss: 0.1071 - val_Bi-Acc: 0.3455 - 10s/epoch - 38ms/step
Epoch 262/300

Epoch 262: val_loss did not improve from 0.10185
253/253 - 10s - loss: 0.0677 - Bi-Acc: 0.3930 - val_loss: 0.1066 - val_Bi-Acc: 0.3455 - 10s/epoch - 38ms/step
Epoch 263/300

Epoch 263: val_loss did not improve from 0.10185
253/253 - 9s - loss: 0.0677 - Bi-Acc: 0.3920 - val_loss: 0.1060 - val_Bi-Acc: 0.3515 - 9s/epoch - 36ms/step
Epoch 264/300

Epoch 264: val_loss did not improve from 0.10185
253/253 - 9s - loss: 0.0673 - Bi-Acc: 0.3933 - val_loss: 0.1068 - val_Bi-Acc: 0.3455 - 9s/epoch - 37ms/step
Epoch 265/300

Epoch 265: val_loss did not improve from 0.10185
253/253 - 10s - loss: 0.0676 - Bi-Acc: 0.3930 - val_loss: 0.1066 - val_Bi-Acc: 0.3455 - 10s/epoch - 38ms/step
Epoch 266/300

Epoch 266: val_loss did not improve from 0.10185
253/253 - 9s - loss: 0.0670 - Bi-Acc: 0.3934 - val_loss: 0.1057 - val_Bi-Acc: 0.3394 - 9s/epoch - 35ms/step
Epoch 267/300

Epoch 267: val_loss did not improve from 0.10185
253/253 - 9s - loss: 0.0668 - Bi-Acc: 0.3927 - val_loss: 0.1055 - val_Bi-Acc: 0.3515 - 9s/epoch - 36ms/step
Epoch 268/300

Epoch 268: val_loss did not improve from 0.10185
253/253 - 10s - loss: 0.0671 - Bi-Acc: 0.3927 - val_loss: 0.1043 - val_Bi-Acc: 0.3515 - 10s/epoch - 39ms/step
Epoch 269/300

Epoch 269: val_loss did not improve from 0.10185
253/253 - 9s - loss: 0.0668 - Bi-Acc: 0.3931 - val_loss: 0.1058 - val_Bi-Acc: 0.3515 - 9s/epoch - 35ms/step
Epoch 270/300

Epoch 270: val_loss did not improve from 0.10185
253/253 - 9s - loss: 0.0670 - Bi-Acc: 0.3929 - val_loss: 0.1042 - val_Bi-Acc: 0.3576 - 9s/epoch - 37ms/step
Epoch 271/300

Epoch 271: val_loss did not improve from 0.10185
253/253 - 10s - loss: 0.0666 - Bi-Acc: 0.3929 - val_loss: 0.1060 - val_Bi-Acc: 0.3515 - 10s/epoch - 39ms/step
Epoch 272/300

Epoch 272: val_loss did not improve from 0.10185
253/253 - 9s - loss: 0.0665 - Bi-Acc: 0.3931 - val_loss: 0.1084 - val_Bi-Acc: 0.3455 - 9s/epoch - 37ms/step
Epoch 273/300

Epoch 273: val_loss did not improve from 0.10185
253/253 - 9s - loss: 0.0663 - Bi-Acc: 0.3932 - val_loss: 0.1055 - val_Bi-Acc: 0.3455 - 9s/epoch - 35ms/step
Epoch 274/300

Epoch 274: val_loss did not improve from 0.10185
253/253 - 10s - loss: 0.0665 - Bi-Acc: 0.3927 - val_loss: 0.1103 - val_Bi-Acc: 0.3333 - 10s/epoch - 38ms/step
Epoch 275/300

Epoch 275: val_loss did not improve from 0.10185
253/253 - 9s - loss: 0.0666 - Bi-Acc: 0.3931 - val_loss: 0.1054 - val_Bi-Acc: 0.3576 - 9s/epoch - 37ms/step
Epoch 276/300

Epoch 276: val_loss did not improve from 0.10185
253/253 - 9s - loss: 0.0663 - Bi-Acc: 0.3936 - val_loss: 0.1084 - val_Bi-Acc: 0.3515 - 9s/epoch - 35ms/step
Epoch 277/300

Epoch 277: val_loss did not improve from 0.10185
253/253 - 10s - loss: 0.0666 - Bi-Acc: 0.3931 - val_loss: 0.1091 - val_Bi-Acc: 0.3394 - 10s/epoch - 38ms/step
Epoch 278/300

Epoch 278: val_loss did not improve from 0.10185
253/253 - 9s - loss: 0.0659 - Bi-Acc: 0.3936 - val_loss: 0.1063 - val_Bi-Acc: 0.3455 - 9s/epoch - 37ms/step
Epoch 279/300

Epoch 279: val_loss did not improve from 0.10185
253/253 - 9s - loss: 0.0661 - Bi-Acc: 0.3932 - val_loss: 0.1078 - val_Bi-Acc: 0.3455 - 9s/epoch - 35ms/step
Epoch 280/300

Epoch 280: val_loss did not improve from 0.10185
253/253 - 9s - loss: 0.0661 - Bi-Acc: 0.3933 - val_loss: 0.1067 - val_Bi-Acc: 0.3515 - 9s/epoch - 37ms/step
Epoch 281/300

Epoch 281: val_loss did not improve from 0.10185
253/253 - 10s - loss: 0.0659 - Bi-Acc: 0.3935 - val_loss: 0.1043 - val_Bi-Acc: 0.3576 - 10s/epoch - 38ms/step
Epoch 282/300

Epoch 282: val_loss did not improve from 0.10185
253/253 - 9s - loss: 0.0660 - Bi-Acc: 0.3934 - val_loss: 0.1063 - val_Bi-Acc: 0.3455 - 9s/epoch - 37ms/step
Epoch 283/300

Epoch 283: val_loss did not improve from 0.10185
253/253 - 9s - loss: 0.0660 - Bi-Acc: 0.3932 - val_loss: 0.1048 - val_Bi-Acc: 0.3515 - 9s/epoch - 36ms/step
Epoch 284/300

Epoch 284: val_loss did not improve from 0.10185
253/253 - 10s - loss: 0.0655 - Bi-Acc: 0.3933 - val_loss: 0.1070 - val_Bi-Acc: 0.3515 - 10s/epoch - 38ms/step
Epoch 285/300

Epoch 285: val_loss did not improve from 0.10185
253/253 - 9s - loss: 0.0660 - Bi-Acc: 0.3931 - val_loss: 0.1079 - val_Bi-Acc: 0.3394 - 9s/epoch - 36ms/step
Epoch 286/300

Epoch 286: val_loss did not improve from 0.10185
253/253 - 9s - loss: 0.0655 - Bi-Acc: 0.3940 - val_loss: 0.1069 - val_Bi-Acc: 0.3515 - 9s/epoch - 36ms/step
Epoch 287/300

Epoch 287: val_loss did not improve from 0.10185
253/253 - 10s - loss: 0.0654 - Bi-Acc: 0.3935 - val_loss: 0.1071 - val_Bi-Acc: 0.3576 - 10s/epoch - 39ms/step
Epoch 288/300

Epoch 288: val_loss did not improve from 0.10185
253/253 - 9s - loss: 0.0655 - Bi-Acc: 0.3938 - val_loss: 0.1078 - val_Bi-Acc: 0.3515 - 9s/epoch - 36ms/step
Epoch 289/300

Epoch 289: val_loss did not improve from 0.10185
253/253 - 9s - loss: 0.0652 - Bi-Acc: 0.3940 - val_loss: 0.1081 - val_Bi-Acc: 0.3515 - 9s/epoch - 36ms/step
Epoch 290/300

Epoch 290: val_loss did not improve from 0.10185
253/253 - 10s - loss: 0.0653 - Bi-Acc: 0.3938 - val_loss: 0.1078 - val_Bi-Acc: 0.3515 - 10s/epoch - 38ms/step
Epoch 291/300

Epoch 291: val_loss did not improve from 0.10185
253/253 - 10s - loss: 0.0652 - Bi-Acc: 0.3935 - val_loss: 0.1078 - val_Bi-Acc: 0.3394 - 10s/epoch - 38ms/step
Epoch 292/300

Epoch 292: val_loss did not improve from 0.10185
253/253 - 9s - loss: 0.0654 - Bi-Acc: 0.3935 - val_loss: 0.1075 - val_Bi-Acc: 0.3394 - 9s/epoch - 35ms/step
Epoch 293/300

Epoch 293: val_loss did not improve from 0.10185
253/253 - 9s - loss: 0.0653 - Bi-Acc: 0.3937 - val_loss: 0.1086 - val_Bi-Acc: 0.3394 - 9s/epoch - 37ms/step
Epoch 294/300

Epoch 294: val_loss did not improve from 0.10185
253/253 - 10s - loss: 0.0649 - Bi-Acc: 0.3937 - val_loss: 0.1095 - val_Bi-Acc: 0.3455 - 10s/epoch - 38ms/step
Epoch 295/300

Epoch 295: val_loss did not improve from 0.10185
253/253 - 9s - loss: 0.0654 - Bi-Acc: 0.3935 - val_loss: 0.1076 - val_Bi-Acc: 0.3455 - 9s/epoch - 36ms/step
Epoch 296/300

Epoch 296: val_loss did not improve from 0.10185
253/253 - 9s - loss: 0.0646 - Bi-Acc: 0.3938 - val_loss: 0.1062 - val_Bi-Acc: 0.3515 - 9s/epoch - 36ms/step
Epoch 297/300

Epoch 297: val_loss did not improve from 0.10185
253/253 - 10s - loss: 0.0649 - Bi-Acc: 0.3939 - val_loss: 0.1080 - val_Bi-Acc: 0.3455 - 10s/epoch - 38ms/step
Epoch 298/300

Epoch 298: val_loss did not improve from 0.10185
253/253 - 9s - loss: 0.0650 - Bi-Acc: 0.3938 - val_loss: 0.1068 - val_Bi-Acc: 0.3455 - 9s/epoch - 35ms/step
Epoch 299/300

Epoch 299: val_loss did not improve from 0.10185
253/253 - 9s - loss: 0.0650 - Bi-Acc: 0.3935 - val_loss: 0.1059 - val_Bi-Acc: 0.3515 - 9s/epoch - 37ms/step
Epoch 300/300

Epoch 300: val_loss did not improve from 0.10185
253/253 - 10s - loss: 0.0647 - Bi-Acc: 0.3942 - val_loss: 0.1085 - val_Bi-Acc: 0.3455 - 10s/epoch - 38ms/step
6/6 - 0s - 431ms/epoch - 72ms/step
Validation:

Confusion Matrix for 0
True Pos False Neg
[72 12]
False Pos True Neg
[10 71]
Precision: 0.8780487804878049
Recall: 0.8571428571428571
F1 Score for Neg: 0.8658536585365854
F1 Score for Pos: 0.8674698795180722
4/4 - 0s - 127ms/epoch - 32ms/step
Test:

Confusion Matrix for 0
True Pos False Neg
[52 11]
False Pos True Neg
[14 44]
Precision: 0.7878787878787878
Recall: 0.8253968253968254
F1 Score for Neg: 0.7787610619469026
F1 Score for Pos: 0.8062015503875969

Saved
